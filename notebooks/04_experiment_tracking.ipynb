{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Experiment Tracking Tutorial\n",
    "\n",
    "Welcome to the fourth tutorial in our ML Pipeline series! In this notebook, we'll implement comprehensive experiment tracking using MLflow to manage our ML experiments like a professional data science team.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "- Setting up MLflow experiment tracking\n",
    "- Logging parameters, metrics, and artifacts\n",
    "- Comparing experiments across multiple runs\n",
    "- Managing model versions and lifecycle\n",
    "- Creating experiment dashboards and reports\n",
    "- Best practices for production MLOps\n",
    "\n",
    "## üèÜ Learning Objectives\n",
    "- **Track 20+ experiments** across different algorithms and parameters\n",
    "- **Log comprehensive metrics** (accuracy, precision, recall, F1, R¬≤, RMSE)\n",
    "- **Manage model artifacts** (models, plots, reports)\n",
    "- **Compare experiment results** systematically\n",
    "- **Implement model registry** for production deployment\n",
    "- **Create experiment reports** for stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNIVERSAL SETUP - Works on all PCs and environments\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Navigate to project root if we're in notebooks directory\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "    print(f\"üìÅ Changed to project root: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"üìÅ Already in project root: {os.getcwd()}\")\n",
    "\n",
    "# Add src to Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    print(f\"üì¶ Added to Python path: {src_path}\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "import shutil\n",
    "\n",
    "# MLflow imports\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    print(\"‚úÖ MLflow imported successfully\")\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è MLflow not available: {e}\")\n",
    "    print(\"üí° Install with: pip install mlflow\")\n",
    "    MLFLOW_AVAILABLE = False\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "import uuid  # Add this to your imports\n",
    "\n",
    "\n",
    "# Configure plotting\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('seaborn')  # Fallback for older versions\n",
    "\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"‚úÖ Setup completed successfully!\")\n",
    "if MLFLOW_AVAILABLE:\n",
    "    print(f\"üß™ MLflow version: {mlflow.__version__}\")\n",
    "print(f\"üìä Scikit-learn version: {__import__('sklearn').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ MLflow Setup and Configuration\n",
    "\n",
    "Let's set up MLflow for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLflowExperimentTracker:\n",
    "    \"\"\"Comprehensive MLflow experiment tracking system\"\"\"\n",
    "    \n",
    "    def __init__(self, tracking_uri=None, experiment_name=None):\n",
    "        \"\"\"Initialize MLflow experiment tracker\"\"\"\n",
    "        self.mlflow_available = MLFLOW_AVAILABLE\n",
    "        self.experiments = {}\n",
    "        self.runs = {}\n",
    "        \n",
    "        if not self.mlflow_available:\n",
    "            print(\"‚ö†Ô∏è MLflow not available - using mock tracking\")\n",
    "            self.mock_mode = True\n",
    "            return\n",
    "        \n",
    "        self.mock_mode = False\n",
    "        \n",
    "        # Set up tracking URI\n",
    "        if tracking_uri is None:\n",
    "            mlruns_path = Path.cwd() / \"mlruns\"\n",
    "            mlruns_path.mkdir(exist_ok=True)\n",
    "            # Use proper file URI format for Windows\n",
    "            tracking_uri = mlruns_path.as_uri()  # This creates proper file:// URI\n",
    "\n",
    "        mlflow.set_tracking_uri(tracking_uri)\n",
    "        \n",
    "        print(f\"üß™ MLflow tracking URI: {tracking_uri}\")\n",
    "        \n",
    "        # Create experiments directory\n",
    "        mlruns_dir = Path(\"mlruns\")\n",
    "        mlruns_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(\"‚úÖ MLflow experiment tracker initialized!\")\n",
    "    \n",
    "    def create_experiment(self, experiment_name, description=None):\n",
    "        \"\"\"Create a new MLflow experiment\"\"\"\n",
    "        if self.mock_mode:\n",
    "            print(f\"üß™ [MOCK] Created experiment: {experiment_name}\")\n",
    "            self.experiments[experiment_name] = {'id': str(uuid.uuid4())[:8], 'runs': []}\n",
    "            return self.experiments[experiment_name]['id']\n",
    "        \n",
    "        try:\n",
    "            # Try to get existing experiment\n",
    "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "            if experiment:\n",
    "                print(f\"‚úÖ Using existing experiment: {experiment_name} (ID: {experiment.experiment_id})\")\n",
    "                return experiment.experiment_id\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(\n",
    "                name=experiment_name,\n",
    "                artifact_location=str(Path(\"mlruns\") / \"artifacts\" / experiment_name)\n",
    "            )\n",
    "            print(f\"‚úÖ Created new experiment: {experiment_name} (ID: {experiment_id})\")\n",
    "            \n",
    "            # Add description if provided\n",
    "            if description:\n",
    "                try:\n",
    "                    self.client.update_experiment(experiment_id, description=description)\n",
    "                except:\n",
    "                    pass  # Description update not critical\n",
    "            \n",
    "            return experiment_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error creating experiment: {e}\")\n",
    "            print(\"üîÑ Using default experiment\")\n",
    "            return \"0\"  # Default experiment ID\n",
    "    \n",
    "    def start_run(self, experiment_name, run_name=None):\n",
    "        \"\"\"Start a new MLflow run\"\"\"\n",
    "        if self.mock_mode:\n",
    "            run_id = str(uuid.uuid4())[:8]\n",
    "            print(f\"üß™ [MOCK] Started run: {run_name or run_id}\")\n",
    "            return {'run_id': run_id, 'mock': True}\n",
    "        \n",
    "        try:\n",
    "            # Set experiment\n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            \n",
    "            # Start run\n",
    "            run = mlflow.start_run(run_name=run_name)\n",
    "            print(f\"üß™ Started MLflow run: {run_name or run.info.run_id[:8]}\")\n",
    "            return run\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error starting run: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def log_params(self, params, run=None):\n",
    "        \"\"\"Log parameters to MLflow\"\"\"\n",
    "        if self.mock_mode:\n",
    "            print(f\"üß™ [MOCK] Logged params: {list(params.keys())[:3]}...\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            mlflow.log_params(params)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error logging params: {e}\")\n",
    "    \n",
    "    def log_metrics(self, metrics, step=None):\n",
    "        \"\"\"Log metrics to MLflow\"\"\"\n",
    "        if self.mock_mode:\n",
    "            print(f\"üß™ [MOCK] Logged metrics: {list(metrics.keys())[:3]}...\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            mlflow.log_metrics(metrics, step=step)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error logging metrics: {e}\")\n",
    "    \n",
    "    def log_model(self, model, model_name, signature=None):\n",
    "        \"\"\"Log model to MLflow\"\"\"\n",
    "        if self.mock_mode:\n",
    "            print(f\"üß™ [MOCK] Logged model: {model_name}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            mlflow.sklearn.log_model(\n",
    "                model, \n",
    "                model_name,\n",
    "                signature=signature\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error logging model: {e}\")\n",
    "    \n",
    "    def log_artifact(self, artifact_path, artifact_name=None):\n",
    "        \"\"\"Log artifact to MLflow\"\"\"\n",
    "        if self.mock_mode:\n",
    "            print(f\"üß™ [MOCK] Logged artifact: {artifact_name or artifact_path}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            if artifact_name:\n",
    "                mlflow.log_artifact(artifact_path, artifact_name)\n",
    "            else:\n",
    "                mlflow.log_artifact(artifact_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error logging artifact: {e}\")\n",
    "    \n",
    "    def end_run(self):\n",
    "        \"\"\"End current MLflow run\"\"\"\n",
    "        if self.mock_mode:\n",
    "            print(f\"üß™ [MOCK] Ended run\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            mlflow.end_run()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error ending run: {e}\")\n",
    "\n",
    "# Initialize the experiment tracker\n",
    "tracker = MLflowExperimentTracker()\n",
    "\n",
    "# Create main experiments\n",
    "experiments_config = {\n",
    "    \"Titanic_Classification\": \"Titanic passenger survival prediction experiments\",\n",
    "    \"Housing_Regression\": \"Boston housing price prediction experiments\",\n",
    "    \"Model_Comparison\": \"Cross-dataset model comparison experiments\",\n",
    "    \"Hyperparameter_Tuning\": \"Hyperparameter optimization experiments\"\n",
    "}\n",
    "\n",
    "print(\"\\nüß™ Creating MLflow experiments...\")\n",
    "for exp_name, description in experiments_config.items():\n",
    "    tracker.create_experiment(exp_name, description)\n",
    "\n",
    "print(f\"\\n‚úÖ MLflow setup completed! Created {len(experiments_config)} experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load Training Data\n",
    "\n",
    "Let's load our engineered features and trained models from previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data():\n",
    "    \"\"\"Load data for experiment tracking\"\"\"\n",
    "    print(\"üì• Loading experiment data...\")\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Try to load engineered features\n",
    "    feature_paths = {\n",
    "        'titanic': ['data/features/titanic_features.csv', 'data/raw/titanic.csv'],\n",
    "        'housing': ['data/features/housing_features.csv', 'data/raw/housing.csv']\n",
    "    }\n",
    "    \n",
    "    for dataset_name, paths in feature_paths.items():\n",
    "        loaded = False\n",
    "        for path in paths:\n",
    "            if Path(path).exists():\n",
    "                try:\n",
    "                    df = pd.read_csv(path)\n",
    "                    data[dataset_name] = df\n",
    "                    print(f\"‚úÖ Loaded {dataset_name} data from {path}: {df.shape}\")\n",
    "                    loaded = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error loading {path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if not loaded:\n",
    "            print(f\"‚ùå Could not load {dataset_name} data\")\n",
    "    \n",
    "    # Try to load existing trained models\n",
    "    models_dir = Path(\"trained_models\")\n",
    "    if models_dir.exists():\n",
    "        model_files = list(models_dir.glob(\"*.joblib\"))\n",
    "        print(f\"üì¶ Found {len(model_files)} existing trained models\")\n",
    "        \n",
    "        # Load model registry if available\n",
    "        registry_file = models_dir / \"model_registry.json\"\n",
    "        if registry_file.exists():\n",
    "            try:\n",
    "                with open(registry_file, 'r') as f:\n",
    "                    model_registry = json.load(f)\n",
    "                data['model_registry'] = model_registry\n",
    "                print(f\"‚úÖ Loaded model registry with {len(model_registry)} models\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading model registry: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "experiment_data = load_experiment_data()\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nüìä Experiment Data Summary:\")\n",
    "for key, value in experiment_data.items():\n",
    "    if isinstance(value, pd.DataFrame):\n",
    "        print(f\"   {key}: {value.shape} DataFrame\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"   {key}: {len(value)} items\")\n",
    "    else:\n",
    "        print(f\"   {key}: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Comprehensive Experiment Runner\n",
    "\n",
    "Let's create a comprehensive system to run and track multiple experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveExperimentRunner:\n",
    "    \"\"\"Run and track comprehensive ML experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, tracker):\n",
    "        self.tracker = tracker\n",
    "        self.experiment_results = {}\n",
    "        \n",
    "        # Define model configurations for experiments\n",
    "        self.classification_configs = {\n",
    "            'RandomForest_Basic': {\n",
    "                'model': RandomForestClassifier(random_state=42),\n",
    "                'params': {'n_estimators': 100, 'max_depth': 10}\n",
    "            },\n",
    "            'RandomForest_Tuned': {\n",
    "                'model': RandomForestClassifier(random_state=42),\n",
    "                'params': {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 5}\n",
    "            },\n",
    "            'LogisticRegression_Basic': {\n",
    "                'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "                'params': {'C': 1.0, 'solver': 'lbfgs'}\n",
    "            },\n",
    "            'LogisticRegression_Tuned': {\n",
    "                'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "                'params': {'C': 10.0, 'solver': 'liblinear'}\n",
    "            },\n",
    "            'SVM_Basic': {\n",
    "                'model': SVC(random_state=42, probability=True),\n",
    "                'params': {'C': 1.0, 'kernel': 'rbf'}\n",
    "            },\n",
    "            'SVM_Tuned': {\n",
    "                'model': SVC(random_state=42, probability=True),\n",
    "                'params': {'C': 10.0, 'kernel': 'rbf', 'gamma': 'scale'}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.regression_configs = {\n",
    "            'RandomForest_Basic': {\n",
    "                'model': RandomForestRegressor(random_state=42),\n",
    "                'params': {'n_estimators': 100, 'max_depth': 10}\n",
    "            },\n",
    "            'RandomForest_Tuned': {\n",
    "                'model': RandomForestRegressor(random_state=42),\n",
    "                'params': {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 5}\n",
    "            },\n",
    "            'LinearRegression': {\n",
    "                'model': LinearRegression(),\n",
    "                'params': {}\n",
    "            },\n",
    "            'SVR_Basic': {\n",
    "                'model': SVR(),\n",
    "                'params': {'C': 1.0, 'kernel': 'rbf'}\n",
    "            },\n",
    "            'SVR_Tuned': {\n",
    "                'model': SVR(),\n",
    "                'params': {'C': 10.0, 'kernel': 'rbf', 'gamma': 'scale'}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def prepare_data(self, data, target_col, test_size=0.2):\n",
    "        \"\"\"Prepare data for experiments\"\"\"\n",
    "        # Handle missing values\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Fill missing values\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype in ['object', 'category']:\n",
    "                if col != target_col:\n",
    "                    df[col] = pd.Categorical(df[col]).codes\n",
    "            else:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df.drop([target_col], axis=1)\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Handle infinite values\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42,\n",
    "            stratify=y if len(y.unique()) < 20 else None\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, X.columns.tolist()\n",
    "    \n",
    "    def run_classification_experiments(self, data, target_col='Survived', experiment_name='Titanic_Classification'):\n",
    "        \"\"\"Run comprehensive classification experiments\"\"\"\n",
    "        print(f\"üß™ Running classification experiments for {experiment_name}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_test, y_train, y_test, feature_names = self.prepare_data(data, target_col)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for config_name, config in self.classification_configs.items():\n",
    "            print(f\"\\nüîÑ Running {config_name}...\")\n",
    "            \n",
    "            # Start MLflow run\n",
    "            run = self.tracker.start_run(experiment_name, f\"{config_name}_{datetime.now().strftime('%H%M%S')}\")\n",
    "            \n",
    "            try:\n",
    "                # Set model parameters\n",
    "                model = config['model'].__class__(**config['params'], random_state=42)\n",
    "                \n",
    "                # Train model\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = {\n",
    "                    'accuracy': accuracy_score(y_test, y_pred),\n",
    "                    'precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                    'recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                    'f1_score': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                }\n",
    "                \n",
    "                if y_proba is not None:\n",
    "                    try:\n",
    "                        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "                    except:\n",
    "                        metrics['roc_auc'] = 0.0\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "                metrics['cv_mean'] = cv_scores.mean()\n",
    "                metrics['cv_std'] = cv_scores.std()\n",
    "                \n",
    "                # Log to MLflow\n",
    "                log_params = {\n",
    "                    'model_type': config_name,\n",
    "                    'algorithm': model.__class__.__name__,\n",
    "                    'train_size': len(X_train),\n",
    "                    'test_size': len(X_test),\n",
    "                    'n_features': len(feature_names),\n",
    "                    **config['params']\n",
    "                }\n",
    "                \n",
    "                self.tracker.log_params(log_params)\n",
    "                self.tracker.log_metrics(metrics)\n",
    "                self.tracker.log_model(model, f\"model_{config_name.lower()}\")\n",
    "                \n",
    "                # Create and log confusion matrix plot\n",
    "                self.create_confusion_matrix_plot(y_test, y_pred, config_name)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'config_name': config_name,\n",
    "                    'model': model,\n",
    "                    'metrics': metrics,\n",
    "                    'params': config['params'],\n",
    "                    'run_id': run.info.run_id if run and not self.tracker.mock_mode else 'mock_' + str(uuid.uuid4())[:8]\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"   ‚úÖ {config_name}: Accuracy = {metrics['accuracy']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {config_name} failed: {e}\")\n",
    "            \n",
    "            finally:\n",
    "                self.tracker.end_run()\n",
    "        \n",
    "        self.experiment_results[experiment_name] = results\n",
    "        return results\n",
    "    \n",
    "    def run_regression_experiments(self, data, target_col='MEDV', experiment_name='Housing_Regression'):\n",
    "        \"\"\"Run comprehensive regression experiments\"\"\"\n",
    "        print(f\"üß™ Running regression experiments for {experiment_name}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_test, y_train, y_test, feature_names = self.prepare_data(data, target_col)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for config_name, config in self.regression_configs.items():\n",
    "            print(f\"\\nüîÑ Running {config_name}...\")\n",
    "            \n",
    "            # Start MLflow run\n",
    "            run = self.tracker.start_run(experiment_name, f\"{config_name}_{datetime.now().strftime('%H%M%S')}\")\n",
    "            \n",
    "            try:\n",
    "                # Set model parameters\n",
    "                if config['params']:\n",
    "                    model = config['model'].__class__(**config['params'])\n",
    "                    if hasattr(model, 'random_state'):\n",
    "                        model.random_state = 42\n",
    "                else:\n",
    "                    model = config['model']\n",
    "                \n",
    "                # Train model\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = {\n",
    "                    'r2_score': r2_score(y_test, y_pred),\n",
    "                    'mse': mean_squared_error(y_test, y_pred),\n",
    "                    'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                    'mae': mean_absolute_error(y_test, y_pred)\n",
    "                }\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "                metrics['cv_r2_mean'] = cv_scores.mean()\n",
    "                metrics['cv_r2_std'] = cv_scores.std()\n",
    "                \n",
    "                # Log to MLflow\n",
    "                log_params = {\n",
    "                    'model_type': config_name,\n",
    "                    'algorithm': model.__class__.__name__,\n",
    "                    'train_size': len(X_train),\n",
    "                    'test_size': len(X_test),\n",
    "                    'n_features': len(feature_names),\n",
    "                    **config['params']\n",
    "                }\n",
    "                \n",
    "                self.tracker.log_params(log_params)\n",
    "                self.tracker.log_metrics(metrics)\n",
    "                self.tracker.log_model(model, f\"model_{config_name.lower()}\")\n",
    "                \n",
    "                # Create and log prediction plot\n",
    "                self.create_regression_plot(y_test, y_pred, config_name)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'config_name': config_name,\n",
    "                    'model': model,\n",
    "                    'metrics': metrics,\n",
    "                    'params': config['params'],\n",
    "                    'run_id': run.info.run_id if run and not self.tracker.mock_mode else 'mock_' + str(uuid.uuid4())[:8]\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"   ‚úÖ {config_name}: R¬≤ = {metrics['r2_score']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {config_name} failed: {e}\")\n",
    "            \n",
    "            finally:\n",
    "                self.tracker.end_run()\n",
    "        \n",
    "        self.experiment_results[experiment_name] = results\n",
    "        return results\n",
    "    \n",
    "    def create_confusion_matrix_plot(self, y_true, y_pred, model_name):\n",
    "        \"\"\"Create and save confusion matrix plot\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'Confusion Matrix - {model_name}')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            \n",
    "            # Save plot\n",
    "            plot_path = f\"confusion_matrix_{model_name.lower()}.png\"\n",
    "            plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Log to MLflow\n",
    "            self.tracker.log_artifact(plot_path)\n",
    "            \n",
    "            # Clean up\n",
    "            if Path(plot_path).exists():\n",
    "                Path(plot_path).unlink()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error creating confusion matrix plot: {e}\")\n",
    "    \n",
    "    def create_regression_plot(self, y_true, y_pred, model_name):\n",
    "        \"\"\"Create and save regression prediction plot\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(y_true, y_pred, alpha=0.6)\n",
    "            plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "            plt.xlabel('Actual Values')\n",
    "            plt.ylabel('Predicted Values')\n",
    "            plt.title(f'Actual vs Predicted - {model_name}')\n",
    "            \n",
    "            # Save plot\n",
    "            plot_path = f\"regression_plot_{model_name.lower()}.png\"\n",
    "            plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Log to MLflow\n",
    "            self.tracker.log_artifact(plot_path)\n",
    "            \n",
    "            # Clean up\n",
    "            if Path(plot_path).exists():\n",
    "                Path(plot_path).unlink()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error creating regression plot: {e}\")\n",
    "\n",
    "# Initialize experiment runner\n",
    "experiment_runner = ComprehensiveExperimentRunner(tracker)\n",
    "print(\"‚úÖ Comprehensive experiment runner initialized!\")\n",
    "print(f\"üìä Classification configs: {len(experiment_runner.classification_configs)}\")\n",
    "print(f\"üìä Regression configs: {len(experiment_runner.regression_configs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö¢ Run Titanic Classification Experiments\n",
    "\n",
    "Let's run comprehensive experiments on the Titanic dataset and track everything with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Titanic classification experiments\n",
    "if 'titanic' in experiment_data:\n",
    "    print(\"üö¢ TITANIC CLASSIFICATION EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    titanic_results = experiment_runner.run_classification_experiments(\n",
    "        experiment_data['titanic'], \n",
    "        target_col='Survived',\n",
    "        experiment_name='Titanic_Classification'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Titanic experiments completed! Ran {len(titanic_results)} experiments.\")\n",
    "    \n",
    "    # Display results summary\n",
    "    print(\"\\nüìä Titanic Results Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<25} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for result in sorted(titanic_results, key=lambda x: x['metrics']['accuracy'], reverse=True):\n",
    "        metrics = result['metrics']\n",
    "        print(f\"{result['config_name']:<25} {metrics['accuracy']:<10.4f} \"\n",
    "              f\"{metrics['precision']:<10.4f} {metrics['recall']:<10.4f} {metrics['f1_score']:<10.4f}\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_titanic = max(titanic_results, key=lambda x: x['metrics']['accuracy'])\n",
    "    print(f\"\\nüèÜ Best Titanic Model: {best_titanic['config_name']} (Accuracy: {best_titanic['metrics']['accuracy']:.4f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Titanic data not available for experiments\")\n",
    "    titanic_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè† Run Housing Regression Experiments\n",
    "\n",
    "Now let's run comprehensive experiments on the Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Housing regression experiments\n",
    "if 'housing' in experiment_data:\n",
    "    print(\"üè† HOUSING REGRESSION EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    housing_results = experiment_runner.run_regression_experiments(\n",
    "        experiment_data['housing'], \n",
    "        target_col='MEDV',\n",
    "        experiment_name='Housing_Regression'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Housing experiments completed! Ran {len(housing_results)} experiments.\")\n",
    "    \n",
    "    # Display results summary\n",
    "    print(\"\\nüìä Housing Results Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<25} {'R¬≤ Score':<10} {'RMSE':<10} {'MAE':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for result in sorted(housing_results, key=lambda x: x['metrics']['r2_score'], reverse=True):\n",
    "        metrics = result['metrics']\n",
    "        print(f\"{result['config_name']:<25} {metrics['r2_score']:<10.4f} \"\n",
    "              f\"{metrics['rmse']:<10.4f} {metrics['mae']:<10.4f}\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_housing = max(housing_results, key=lambda x: x['metrics']['r2_score'])\n",
    "    print(f\"\\nüèÜ Best Housing Model: {best_housing['config_name']} (R¬≤: {best_housing['metrics']['r2_score']:.4f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Housing data not available for experiments\")\n",
    "    housing_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Experiment Analysis and Visualization\n",
    "\n",
    "Let's create comprehensive visualizations and analysis of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_visualizations(experiment_results):\n",
    "    \"\"\"Create comprehensive experiment visualizations\"\"\"\n",
    "    print(\"üìä Creating experiment visualizations...\")\n",
    "    \n",
    "    if not experiment_results:\n",
    "        print(\"‚ö†Ô∏è No experiment results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots for each experiment\n",
    "    n_experiments = len(experiment_results)\n",
    "    fig, axes = plt.subplots(2, n_experiments, figsize=(6*n_experiments, 12))\n",
    "    \n",
    "    if n_experiments == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    col_idx = 0\n",
    "    \n",
    "    for exp_name, results in experiment_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "        \n",
    "        # Determine if classification or regression\n",
    "        is_classification = 'accuracy' in results[0]['metrics']\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        model_names = [r['config_name'] for r in results]\n",
    "        \n",
    "        if is_classification:\n",
    "            primary_scores = [r['metrics']['accuracy'] for r in results]\n",
    "            secondary_scores = [r['metrics']['f1_score'] for r in results]\n",
    "            primary_label = 'Accuracy'\n",
    "            secondary_label = 'F1-Score'\n",
    "        else:\n",
    "            primary_scores = [r['metrics']['r2_score'] for r in results]\n",
    "            secondary_scores = [r['metrics']['rmse'] for r in results]\n",
    "            primary_label = 'R¬≤ Score'\n",
    "            secondary_label = 'RMSE'\n",
    "        \n",
    "        # Primary metric plot\n",
    "        ax1 = axes[0, col_idx]\n",
    "        bars1 = ax1.bar(range(len(model_names)), primary_scores, \n",
    "                       color=plt.cm.Set3(np.linspace(0, 1, len(model_names))))\n",
    "        ax1.set_title(f'{exp_name.replace(\"_\", \" \")} - {primary_label}')\n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel(primary_label)\n",
    "        ax1.set_xticks(range(len(model_names)))\n",
    "        ax1.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=0, ha='center')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars1, primary_scores):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + max(primary_scores)*0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Secondary metric plot\n",
    "        ax2 = axes[1, col_idx]\n",
    "        bars2 = ax2.bar(range(len(model_names)), secondary_scores,\n",
    "                       color=plt.cm.Set2(np.linspace(0, 1, len(model_names))))\n",
    "        ax2.set_title(f'{exp_name.replace(\"_\", \" \")} - {secondary_label}')\n",
    "        ax2.set_xlabel('Models')\n",
    "        ax2.set_ylabel(secondary_label)\n",
    "        ax2.set_xticks(range(len(model_names)))\n",
    "        ax2.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=0, ha='center')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars2, secondary_scores):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(secondary_scores)*0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        col_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create cross-validation comparison\n",
    "    print(\"\\nüìä Cross-Validation Performance Comparison:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_experiments, figsize=(6*n_experiments, 6))\n",
    "    if n_experiments == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    col_idx = 0\n",
    "    for exp_name, results in experiment_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "        \n",
    "        model_names = [r['config_name'] for r in results]\n",
    "        \n",
    "        # Get CV scores\n",
    "        if 'cv_mean' in results[0]['metrics']:\n",
    "            cv_means = [r['metrics']['cv_mean'] for r in results]\n",
    "            cv_stds = [r['metrics']['cv_std'] for r in results]\n",
    "            cv_label = 'CV Accuracy'\n",
    "        else:\n",
    "            cv_means = [r['metrics']['cv_r2_mean'] for r in results]\n",
    "            cv_stds = [r['metrics']['cv_r2_std'] for r in results]\n",
    "            cv_label = 'CV R¬≤ Score'\n",
    "        \n",
    "        ax = axes[col_idx]\n",
    "        bars = ax.bar(range(len(model_names)), cv_means, yerr=cv_stds, capsize=5,\n",
    "                     color=plt.cm.Pastel1(np.linspace(0, 1, len(model_names))))\n",
    "        ax.set_title(f'{exp_name.replace(\"_\", \" \")} - {cv_label}')\n",
    "        ax.set_xlabel('Models')\n",
    "        ax.set_ylabel(cv_label)\n",
    "        ax.set_xticks(range(len(model_names)))\n",
    "        ax.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=0, ha='center')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, mean, std in zip(bars, cv_means, cv_stds):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + std + max(cv_means)*0.01,\n",
    "                   f'{mean:.3f}¬±{std:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        col_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "if experiment_runner.experiment_results:\n",
    "    create_experiment_visualizations(experiment_runner.experiment_results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No experiment results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Model Registry Management\n",
    "\n",
    "Let's implement model registry functionality to manage our best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLflowModelRegistry:\n",
    "    \"\"\"MLflow Model Registry management\"\"\"\n",
    "    \n",
    "    def __init__(self, tracker):\n",
    "        self.tracker = tracker\n",
    "        self.registered_models = {}\n",
    "    \n",
    "    def register_best_models(self, experiment_results):\n",
    "        \"\"\"Register best models from experiments\"\"\"\n",
    "        print(\"üèõÔ∏è Registering best models to model registry...\")\n",
    "        \n",
    "        for exp_name, results in experiment_results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            \n",
    "            # Find best model\n",
    "            if 'accuracy' in results[0]['metrics']:\n",
    "                best_result = max(results, key=lambda x: x['metrics']['accuracy'])\n",
    "                metric_name = 'accuracy'\n",
    "                metric_value = best_result['metrics']['accuracy']\n",
    "            else:\n",
    "                best_result = max(results, key=lambda x: x['metrics']['r2_score'])\n",
    "                metric_name = 'r2_score'\n",
    "                metric_value = best_result['metrics']['r2_score']\n",
    "            \n",
    "            # Create model name\n",
    "            model_name = f\"{exp_name}_Best_Model\"\n",
    "            \n",
    "            # Register model (mock implementation)\n",
    "            model_info = {\n",
    "                'name': model_name,\n",
    "                'version': 1,\n",
    "                'stage': 'Staging',\n",
    "                'run_id': best_result['run_id'],\n",
    "                'config_name': best_result['config_name'],\n",
    "                'best_metric': metric_name,\n",
    "                'best_value': metric_value,\n",
    "                'metrics': best_result['metrics'],\n",
    "                'params': best_result['params'],\n",
    "                'registered_at': datetime.now().isoformat(),\n",
    "                'description': f\"Best {exp_name.replace('_', ' ')} model with {metric_name}: {metric_value:.4f}\"\n",
    "            }\n",
    "            \n",
    "            self.registered_models[model_name] = model_info\n",
    "            \n",
    "            print(f\"‚úÖ Registered {model_name}:\")\n",
    "            print(f\"   Algorithm: {best_result['config_name']}\")\n",
    "            print(f\"   Best {metric_name}: {metric_value:.4f}\")\n",
    "            print(f\"   Stage: Staging\")\n",
    "    \n",
    "    def promote_to_production(self, model_name, min_accuracy=0.85, min_r2=0.65):\n",
    "        \"\"\"Promote model to production if it meets criteria\"\"\"\n",
    "        if model_name not in self.registered_models:\n",
    "            print(f\"‚ùå Model {model_name} not found in registry\")\n",
    "            return False\n",
    "        \n",
    "        model_info = self.registered_models[model_name]\n",
    "        \n",
    "        # Check promotion criteria\n",
    "        can_promote = False\n",
    "        \n",
    "        if model_info['best_metric'] == 'accuracy':\n",
    "            can_promote = model_info['best_value'] >= min_accuracy\n",
    "            threshold = min_accuracy\n",
    "        elif model_info['best_metric'] == 'r2_score':\n",
    "            can_promote = model_info['best_value'] >= min_r2\n",
    "            threshold = min_r2\n",
    "        \n",
    "        if can_promote:\n",
    "            model_info['stage'] = 'Production'\n",
    "            model_info['promoted_at'] = datetime.now().isoformat()\n",
    "            print(f\"üöÄ Promoted {model_name} to Production\")\n",
    "            print(f\"   {model_info['best_metric']}: {model_info['best_value']:.4f} >= {threshold}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {model_name} does not meet promotion criteria\")\n",
    "            print(f\"   {model_info['best_metric']}: {model_info['best_value']:.4f} < {threshold}\")\n",
    "            return False\n",
    "    \n",
    "    def get_production_models(self):\n",
    "        \"\"\"Get all production models\"\"\"\n",
    "        return {name: info for name, info in self.registered_models.items() \n",
    "                if info['stage'] == 'Production'}\n",
    "    \n",
    "    def create_model_registry_report(self):\n",
    "        \"\"\"Create comprehensive model registry report\"\"\"\n",
    "        print(\"üìÑ Creating model registry report...\")\n",
    "        \n",
    "        report_lines = []\n",
    "        report_lines.append(\"# MLflow Model Registry Report\\n\\n\")\n",
    "        report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        # Summary\n",
    "        total_models = len(self.registered_models)\n",
    "        production_models = len(self.get_production_models())\n",
    "        staging_models = total_models - production_models\n",
    "        \n",
    "        report_lines.append(\"## üìä Registry Summary\\n\\n\")\n",
    "        report_lines.append(f\"- **Total Models**: {total_models}\\n\")\n",
    "        report_lines.append(f\"- **Production Models**: {production_models}\\n\")\n",
    "        report_lines.append(f\"- **Staging Models**: {staging_models}\\n\\n\")\n",
    "        \n",
    "        # Model details\n",
    "        report_lines.append(\"## üèõÔ∏è Registered Models\\n\\n\")\n",
    "        \n",
    "        for model_name, info in self.registered_models.items():\n",
    "            stage_emoji = \"üöÄ\" if info['stage'] == 'Production' else \"üß™\"\n",
    "            report_lines.append(f\"### {stage_emoji} {model_name}\\n\\n\")\n",
    "            report_lines.append(f\"- **Stage**: {info['stage']}\\n\")\n",
    "            report_lines.append(f\"- **Algorithm**: {info['config_name']}\\n\")\n",
    "            report_lines.append(f\"- **Best Metric**: {info['best_metric']} = {info['best_value']:.4f}\\n\")\n",
    "            report_lines.append(f\"- **Registered**: {info['registered_at']}\\n\")\n",
    "            \n",
    "            if 'promoted_at' in info:\n",
    "                report_lines.append(f\"- **Promoted**: {info['promoted_at']}\\n\")\n",
    "            \n",
    "            report_lines.append(f\"- **Description**: {info['description']}\\n\\n\")\n",
    "            \n",
    "            # Performance metrics\n",
    "            report_lines.append(\"**Performance Metrics:**\\n\")\n",
    "            for metric, value in info['metrics'].items():\n",
    "                report_lines.append(f\"- {metric}: {value:.4f}\\n\")\n",
    "            report_lines.append(\"\\n\")\n",
    "        \n",
    "        # Save report\n",
    "        report_file = \"model_registry_report.md\"\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(report_lines)\n",
    "        \n",
    "        print(f\"‚úÖ Model registry report saved: {report_file}\")\n",
    "        return report_file\n",
    "\n",
    "# Initialize model registry\n",
    "model_registry = MLflowModelRegistry(tracker)\n",
    "\n",
    "# Register best models\n",
    "if experiment_runner.experiment_results:\n",
    "    model_registry.register_best_models(experiment_runner.experiment_results)\n",
    "    \n",
    "    # Try to promote models to production\n",
    "    print(\"\\nüöÄ Evaluating models for production promotion...\")\n",
    "    for model_name in model_registry.registered_models.keys():\n",
    "        model_registry.promote_to_production(model_name)\n",
    "    \n",
    "    # Create registry report\n",
    "    model_registry.create_model_registry_report()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model registry management completed!\")\n",
    "    print(f\"üìä Total registered models: {len(model_registry.registered_models)}\")\n",
    "    print(f\"üöÄ Production models: {len(model_registry.get_production_models())}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No experiment results available for model registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Experiment Comparison and Analysis\n",
    "\n",
    "Let's create comprehensive experiment comparison and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_experiment_analysis(experiment_results, model_registry):\n",
    "    \"\"\"Create comprehensive experiment analysis\"\"\"\n",
    "    print(\"üìà COMPREHENSIVE EXPERIMENT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not experiment_results:\n",
    "        print(\"‚ö†Ô∏è No experiment results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_experiments = sum(len(results) for results in experiment_results.values())\n",
    "    total_datasets = len(experiment_results)\n",
    "    \n",
    "    print(f\"üìä Overall Statistics:\")\n",
    "    print(f\"   Datasets: {total_datasets}\")\n",
    "    print(f\"   Total Experiments: {total_experiments}\")\n",
    "    print(f\"   Registered Models: {len(model_registry.registered_models)}\")\n",
    "    print(f\"   Production Models: {len(model_registry.get_production_models())}\")\n",
    "    \n",
    "    # Performance analysis by dataset\n",
    "    print(f\"\\nüìä Performance Analysis by Dataset:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    analysis_data = []\n",
    "    \n",
    "    for exp_name, results in experiment_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüéØ {exp_name.replace('_', ' ')}:\")\n",
    "        \n",
    "        # Determine metric type\n",
    "        is_classification = 'accuracy' in results[0]['metrics']\n",
    "        primary_metric = 'accuracy' if is_classification else 'r2_score'\n",
    "        \n",
    "        # Extract performance data\n",
    "        performances = [r['metrics'][primary_metric] for r in results]\n",
    "        model_names = [r['config_name'] for r in results]\n",
    "        \n",
    "        # Statistics\n",
    "        best_performance = max(performances)\n",
    "        worst_performance = min(performances)\n",
    "        avg_performance = np.mean(performances)\n",
    "        std_performance = np.std(performances)\n",
    "        \n",
    "        best_model = results[performances.index(best_performance)]['config_name']\n",
    "        worst_model = results[performances.index(worst_performance)]['config_name']\n",
    "        \n",
    "        print(f\"   Best: {best_model} ({primary_metric}: {best_performance:.4f})\")\n",
    "        print(f\"   Worst: {worst_model} ({primary_metric}: {worst_performance:.4f})\")\n",
    "        print(f\"   Average: {avg_performance:.4f} ¬± {std_performance:.4f}\")\n",
    "        print(f\"   Range: {worst_performance:.4f} - {best_performance:.4f}\")\n",
    "        \n",
    "        # Store for overall analysis\n",
    "        analysis_data.append({\n",
    "            'dataset': exp_name,\n",
    "            'task_type': 'Classification' if is_classification else 'Regression',\n",
    "            'best_model': best_model,\n",
    "            'best_performance': best_performance,\n",
    "            'avg_performance': avg_performance,\n",
    "            'std_performance': std_performance,\n",
    "            'n_experiments': len(results)\n",
    "        })\n",
    "    \n",
    "    # Algorithm performance analysis\n",
    "    print(f\"\\nü§ñ Algorithm Performance Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Collect all algorithm performances\n",
    "    algorithm_performances = {}\n",
    "    \n",
    "    for exp_name, results in experiment_results.items():\n",
    "        is_classification = 'accuracy' in results[0]['metrics']\n",
    "        primary_metric = 'accuracy' if is_classification else 'r2_score'\n",
    "        \n",
    "        for result in results:\n",
    "            algo_name = result['config_name'].split('_')[0]  # Get base algorithm name\n",
    "            if algo_name not in algorithm_performances:\n",
    "                algorithm_performances[algo_name] = []\n",
    "            algorithm_performances[algo_name].append(result['metrics'][primary_metric])\n",
    "    \n",
    "    # Analyze algorithm performance\n",
    "    for algo, performances in algorithm_performances.items():\n",
    "        avg_perf = np.mean(performances)\n",
    "        std_perf = np.std(performances)\n",
    "        n_experiments = len(performances)\n",
    "        \n",
    "        print(f\"   {algo}: {avg_perf:.4f} ¬± {std_perf:.4f} ({n_experiments} experiments)\")\n",
    "    \n",
    "    # Target achievement analysis\n",
    "    print(f\"\\nüéØ Target Achievement Analysis:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    targets = {\n",
    "        'Titanic_Classification': {'target': 0.894, 'metric': 'accuracy'},\n",
    "        'Housing_Regression': {'target': 0.681, 'metric': 'r2_score'}\n",
    "    }\n",
    "    \n",
    "    achievements = []\n",
    "    \n",
    "    for exp_name, results in experiment_results.items():\n",
    "        if exp_name in targets:\n",
    "            target_info = targets[exp_name]\n",
    "            target_value = target_info['target']\n",
    "            metric_name = target_info['metric']\n",
    "            \n",
    "            # Find best performance\n",
    "            best_result = max(results, key=lambda x: x['metrics'][metric_name])\n",
    "            best_performance = best_result['metrics'][metric_name]\n",
    "            \n",
    "            achievement_pct = (best_performance / target_value) * 100\n",
    "            status = \"‚úÖ ACHIEVED\" if best_performance >= target_value else \"‚ö†Ô∏è CLOSE\" if achievement_pct >= 95 else \"‚ùå NEEDS WORK\"\n",
    "            \n",
    "            print(f\"   {exp_name.replace('_', ' ')}:\")\n",
    "            print(f\"     Target: {target_value:.3f}\")\n",
    "            print(f\"     Achieved: {best_performance:.3f}\")\n",
    "            print(f\"     Achievement: {achievement_pct:.1f}% - {status}\")\n",
    "            print(f\"     Best Model: {best_result['config_name']}\")\n",
    "            \n",
    "            achievements.append({\n",
    "                'dataset': exp_name,\n",
    "                'target': target_value,\n",
    "                'achieved': best_performance,\n",
    "                'achievement_pct': achievement_pct,\n",
    "                'status': status,\n",
    "                'best_model': best_result['config_name']\n",
    "            })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    if analysis_data:\n",
    "        analysis_df = pd.DataFrame(analysis_data)\n",
    "        print(f\"\\nüìä Experiment Summary Table:\")\n",
    "        print(analysis_df.to_string(index=False))\n",
    "    \n",
    "    return analysis_data, achievements\n",
    "\n",
    "# Run comprehensive analysis\n",
    "if experiment_runner.experiment_results:\n",
    "    analysis_data, achievements = create_comprehensive_experiment_analysis(\n",
    "        experiment_runner.experiment_results, \n",
    "        model_registry\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No experiment results available for analysis\")\n",
    "    analysis_data, achievements = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Generate Comprehensive Experiment Report\n",
    "\n",
    "Let's create a comprehensive report of all our experiments and findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_experiment_report(experiment_results, model_registry, analysis_data, achievements):\n",
    "    \"\"\"Generate comprehensive experiment report\"\"\"\n",
    "    print(\"üìÑ Generating comprehensive experiment report...\")\n",
    "    \n",
    "    report_lines = []\n",
    "    report_lines.append(\"# üß™ MLflow Experiment Tracking Report\\n\\n\")\n",
    "    report_lines.append(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    total_experiments = sum(len(results) for results in experiment_results.values())\n",
    "    total_datasets = len(experiment_results)\n",
    "    successful_targets = len([a for a in achievements if 'ACHIEVED' in a['status']])\n",
    "    \n",
    "    report_lines.append(\"## üìä Executive Summary\\n\\n\")\n",
    "    report_lines.append(f\"- **Total Experiments Run**: {total_experiments}\\n\")\n",
    "    report_lines.append(f\"- **Datasets Analyzed**: {total_datasets}\\n\")\n",
    "    report_lines.append(f\"- **Models Registered**: {len(model_registry.registered_models)}\\n\")\n",
    "    report_lines.append(f\"- **Production Models**: {len(model_registry.get_production_models())}\\n\")\n",
    "    report_lines.append(f\"- **Targets Achieved**: {successful_targets}/{len(achievements)}\\n\\n\")\n",
    "    \n",
    "    # Experiment Results by Dataset\n",
    "    report_lines.append(\"## üéØ Experiment Results by Dataset\\n\\n\")\n",
    "    \n",
    "    for exp_name, results in experiment_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "        \n",
    "        report_lines.append(f\"### {exp_name.replace('_', ' ')}\\n\\n\")\n",
    "        \n",
    "        # Task type\n",
    "        is_classification = 'accuracy' in results[0]['metrics']\n",
    "        task_type = 'Classification' if is_classification else 'Regression'\n",
    "        report_lines.append(f\"**Task Type:** {task_type}\\n\\n\")\n",
    "        \n",
    "        # Results table\n",
    "        if is_classification:\n",
    "            report_lines.append(\"| Rank | Model | Accuracy | Precision | Recall | F1-Score | CV Score |\\n\")\n",
    "            report_lines.append(\"|------|-------|----------|-----------|--------|----------|----------|\\n\")\n",
    "            \n",
    "            sorted_results = sorted(results, key=lambda x: x['metrics']['accuracy'], reverse=True)\n",
    "            for i, result in enumerate(sorted_results, 1):\n",
    "                m = result['metrics']\n",
    "                report_lines.append(f\"| {i} | {result['config_name']} | {m['accuracy']:.4f} | \"\n",
    "                                   f\"{m['precision']:.4f} | {m['recall']:.4f} | {m['f1_score']:.4f} | \"\n",
    "                                   f\"{m['cv_mean']:.4f} |\\n\")\n",
    "        else:\n",
    "            report_lines.append(\"| Rank | Model | R¬≤ Score | RMSE | MAE | CV R¬≤ |\\n\")\n",
    "            report_lines.append(\"|------|-------|----------|------|-----|-------|\\n\")\n",
    "            \n",
    "            sorted_results = sorted(results, key=lambda x: x['metrics']['r2_score'], reverse=True)\n",
    "            for i, result in enumerate(sorted_results, 1):\n",
    "                m = result['metrics']\n",
    "                report_lines.append(f\"| {i} | {result['config_name']} | {m['r2_score']:.4f} | \"\n",
    "                                   f\"{m['rmse']:.4f} | {m['mae']:.4f} | {m['cv_r2_mean']:.4f} |\\n\")\n",
    "        \n",
    "        report_lines.append(\"\\n\")\n",
    "        \n",
    "        # Best model details\n",
    "        best_result = sorted_results[0]\n",
    "        report_lines.append(f\"**üèÜ Best Model:** {best_result['config_name']}\\n\\n\")\n",
    "        report_lines.append(f\"**Parameters:**\\n\")\n",
    "        for param, value in best_result['params'].items():\n",
    "            report_lines.append(f\"- {param}: {value}\\n\")\n",
    "        report_lines.append(\"\\n\")\n",
    "    \n",
    "    # Target Achievement Analysis\n",
    "    report_lines.append(\"## üéØ Target Achievement Analysis\\n\\n\")\n",
    "    \n",
    "    for achievement in achievements:\n",
    "        status_emoji = \"‚úÖ\" if \"ACHIEVED\" in achievement['status'] else \"‚ö†Ô∏è\" if \"CLOSE\" in achievement['status'] else \"‚ùå\"\n",
    "        report_lines.append(f\"### {status_emoji} {achievement['dataset'].replace('_', ' ')}\\n\\n\")\n",
    "        report_lines.append(f\"- **Target**: {achievement['target']:.3f}\\n\")\n",
    "        report_lines.append(f\"- **Achieved**: {achievement['achieved']:.3f}\\n\")\n",
    "        report_lines.append(f\"- **Achievement Rate**: {achievement['achievement_pct']:.1f}%\\n\")\n",
    "        report_lines.append(f\"- **Status**: {achievement['status']}\\n\")\n",
    "        report_lines.append(f\"- **Best Model**: {achievement['best_model']}\\n\\n\")\n",
    "    \n",
    "    # Model Registry Summary\n",
    "    report_lines.append(\"## üèõÔ∏è Model Registry Summary\\n\\n\")\n",
    "    \n",
    "    production_models = model_registry.get_production_models()\n",
    "    \n",
    "    if production_models:\n",
    "        report_lines.append(\"### üöÄ Production Models\\n\\n\")\n",
    "        for model_name, info in production_models.items():\n",
    "            report_lines.append(f\"**{model_name}**\\n\")\n",
    "            report_lines.append(f\"- Algorithm: {info['config_name']}\\n\")\n",
    "            report_lines.append(f\"- Performance: {info['best_metric']} = {info['best_value']:.4f}\\n\")\n",
    "            report_lines.append(f\"- Promoted: {info.get('promoted_at', 'N/A')}\\n\\n\")\n",
    "    else:\n",
    "        report_lines.append(\"No models currently in production.\\n\\n\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report_lines.append(\"## üí° Recommendations\\n\\n\")\n",
    "    \n",
    "    # Generate recommendations based on results\n",
    "    recommendations = []\n",
    "    \n",
    "    # Check if targets were achieved\n",
    "    if successful_targets < len(achievements):\n",
    "        recommendations.append(\"üéØ **Performance Improvement**: Some models did not meet target performance. Consider:\")\n",
    "        recommendations.append(\"   - Additional feature engineering\")\n",
    "        recommendations.append(\"   - Hyperparameter optimization\")\n",
    "        recommendations.append(\"   - Ensemble methods\")\n",
    "        recommendations.append(\"   - More training data\")\n",
    "    \n",
    "    if len(production_models) == 0:\n",
    "        recommendations.append(\"üöÄ **Model Deployment**: No models are currently in production. Consider promoting the best performing models.\")\n",
    "    \n",
    "    recommendations.append(\"üìä **Monitoring**: Implement model monitoring to track performance degradation in production.\")\n",
    "    recommendations.append(\"üîÑ **Continuous Training**: Set up automated retraining pipelines for model updates.\")\n",
    "    recommendations.append(\"üß™ **A/B Testing**: Implement A/B testing framework for model comparison in production.\")\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        report_lines.append(f\"{rec}\\n\\n\")\n",
    "    \n",
    "    # Next Steps\n",
    "    report_lines.append(\"## üöÄ Next Steps\\n\\n\")\n",
    "    report_lines.append(\"1. **Model Deployment**: Deploy production-ready models using the deployment tutorial\\n\")\n",
    "    report_lines.append(\"2. **Monitoring Setup**: Implement model monitoring and alerting\\n\")\n",
    "    report_lines.append(\"3. **Performance Optimization**: Continue hyperparameter tuning for better results\\n\")\n",
    "    report_lines.append(\"4. **Feature Engineering**: Explore additional feature engineering techniques\\n\")\n",
    "    report_lines.append(\"5. **Ensemble Methods**: Combine best models for improved performance\\n\\n\")\n",
    "    \n",
    "    # Technical Details\n",
    "    report_lines.append(\"## üîß Technical Details\\n\\n\")\n",
    "    report_lines.append(f\"- **MLflow Tracking URI**: {tracker.tracking_uri if hasattr(tracker, 'tracking_uri') else 'Mock Mode'}\\n\")\n",
    "    report_lines.append(f\"- **Experiments Created**: {len(experiments_config)}\\n\")\n",
    "    report_lines.append(f\"- **Total Runs**: {total_experiments}\\n\")\n",
    "    report_lines.append(f\"- **Artifacts Logged**: Confusion matrices, regression plots, model files\\n\")\n",
    "    report_lines.append(f\"- **Cross-Validation**: 5-fold CV used for all models\\n\\n\")\n",
    "    \n",
    "    # Save report\n",
    "    report_file = \"comprehensive_experiment_report.md\"\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(report_lines)\n",
    "    \n",
    "    print(f\"‚úÖ Comprehensive experiment report saved: {report_file}\")\n",
    "    return report_file\n",
    "\n",
    "# Generate comprehensive report\n",
    "if experiment_runner.experiment_results:\n",
    "    report_file = generate_comprehensive_experiment_report(\n",
    "        experiment_runner.experiment_results,\n",
    "        model_registry,\n",
    "        analysis_data,\n",
    "        achievements\n",
    "    )\n",
    "    print(f\"\\nüìÑ Report generated: {report_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No experiment results available for report generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed the experiment tracking tutorial! You now understand:\n",
    "\n",
    "‚úÖ **MLflow Setup**: Configured comprehensive experiment tracking  \n",
    "‚úÖ **Experiment Management**: Created and organized multiple experiments  \n",
    "‚úÖ **Parameter Logging**: Tracked all model parameters and hyperparameters  \n",
    "‚úÖ **Metrics Tracking**: Logged comprehensive performance metrics  \n",
    "‚úÖ **Artifact Management**: Saved models, plots, and reports  \n",
    "‚úÖ **Model Registry**: Implemented model versioning and lifecycle management  \n",
    "\n",
    "### üèÜ What We Achieved\n",
    "\n",
    "**üß™ Experiment Tracking:**\n",
    "- **Total Experiments**: 10+ comprehensive experiments\n",
    "- **Datasets**: Titanic (Classification) + Housing (Regression)\n",
    "- **Algorithms**: Random Forest, Logistic Regression, SVM, Linear Regression, SVR\n",
    "- **Metrics Logged**: Accuracy, Precision, Recall, F1, R¬≤, RMSE, MAE, CV scores\n",
    "- **Artifacts**: Confusion matrices, regression plots, model files\n",
    "\n",
    "**üèõÔ∏è Model Registry:**\n",
    "- **Model Registration**: Best models automatically registered\n",
    "- **Lifecycle Management**: Staging ‚Üí Production promotion workflow\n",
    "- **Performance Criteria**: Automated promotion based on performance thresholds\n",
    "- **Model Metadata**: Complete model information and lineage\n",
    "\n",
    "**üìä Analysis & Reporting:**\n",
    "- **Performance Comparison**: Side-by-side model comparison\n",
    "- **Target Achievement**: Tracking against performance goals\n",
    "- **Comprehensive Reports**: Detailed experiment analysis\n",
    "- **Recommendations**: Data-driven improvement suggestions\n",
    "\n",
    "### üîß Key Techniques Mastered\n",
    "\n",
    "1. **MLflow Integration**: Professional experiment tracking setup\n",
    "2. **Systematic Experimentation**: Organized, reproducible experiments\n",
    "3. **Performance Monitoring**: Comprehensive metrics tracking\n",
    "4. **Model Lifecycle**: From experimentation to production\n",
    "5. **Artifact Management**: Organized storage of models and plots\n",
    "6. **Automated Analysis**: Data-driven model comparison and selection\n",
    "\n",
    "### üöÄ Next Tutorial\n",
    "In the final notebook (`05_model_deployment.ipynb`), we'll learn to:\n",
    "- Deploy models to production environments\n",
    "- Create REST APIs for model serving\n",
    "- Implement Docker containerization\n",
    "- Set up monitoring and logging\n",
    "- Create web interfaces for predictions\n",
    "\n",
    "### üí° Practice Exercises\n",
    "Try these exercises to reinforce your learning:\n",
    "1. Add more algorithms (XGBoost, Neural Networks)\n",
    "2. Implement automated hyperparameter optimization\n",
    "3. Create custom metrics for domain-specific evaluation\n",
    "4. Set up automated experiment scheduling\n",
    "\n",
    "### üìÅ Files Created\n",
    "Your experiment tracking artifacts are saved as:\n",
    "- `mlruns/` - MLflow experiment data and artifacts\n",
    "- `comprehensive_experiment_report.md` - Detailed experiment analysis\n",
    "- `model_registry_report.md` - Model registry status\n",
    "- Various plots and visualizations\n",
    "\n",
    "### üéØ MLflow UI Access\n",
    "To view your experiments in the MLflow UI:\n",
    "```bash\n",
    "mlflow ui --backend-store-uri file:///./mlruns\n",
    "```\n",
    "Then open: http://localhost:5000\n",
    "\n",
    "Your experiments are now professionally tracked and ready for production deployment! üéä\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Ready for Model Deployment?**  \n",
    "Run: `jupyter notebook notebooks/05_model_deployment.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
