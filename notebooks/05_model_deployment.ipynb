{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Model Deployment Tutorial\n",
    "\n",
    "Welcome to the final tutorial in our ML Pipeline series! In this notebook, we'll deploy our trained models to production environments using modern deployment strategies and create production-ready APIs.\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "- Creating REST APIs for model serving\n",
    "- Building web interfaces for predictions\n",
    "- Docker containerization for consistent deployment\n",
    "- Model monitoring and logging\n",
    "- Production deployment strategies\n",
    "- Health checks and error handling\n",
    "\n",
    "## 🏆 Deployment Objectives\n",
    "- **REST API**: Create production-ready model serving endpoints\n",
    "- **Web Interface**: Build user-friendly prediction interface\n",
    "- **Docker Container**: Package everything for consistent deployment\n",
    "- **Monitoring**: Implement logging and health checks\n",
    "- **Documentation**: Create API documentation and usage guides\n",
    "- **Testing**: Implement comprehensive testing strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNIVERSAL SETUP - Works on all PCs and environments\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Navigate to project root if we're in notebooks directory\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "    print(f\"📁 Changed to project root: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"📁 Already in project root: {os.getcwd()}\")\n",
    "\n",
    "# Add src to Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    print(f\"📦 Added to Python path: {src_path}\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "import time\n",
    "import threading\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Web framework imports\n",
    "try:\n",
    "    from flask import Flask, request, jsonify, render_template_string, send_from_directory\n",
    "    from werkzeug.serving import make_server\n",
    "    print(\"✅ Flask imported successfully\")\n",
    "    FLASK_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Flask not available: {e}\")\n",
    "    print(\"💡 Install with: pip install flask\")\n",
    "    FLASK_AVAILABLE = False\n",
    "\n",
    "# MLflow imports (optional)\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure plotting\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('seaborn')  # Fallback for older versions\n",
    "\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"✅ Setup completed successfully!\")\n",
    "print(f\"🌐 Flask available: {FLASK_AVAILABLE}\")\n",
    "print(f\"🧪 MLflow available: {MLFLOW_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Load Trained Models\n",
    "\n",
    "Let's load our best trained models from previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"Load and manage trained models for deployment\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dir=\"trained_models\"):\n",
    "        self.models_dir = Path(models_dir)\n",
    "        self.loaded_models = {}\n",
    "        self.model_metadata = {}\n",
    "        self.feature_columns = {}\n",
    "        \n",
    "    def load_available_models(self):\n",
    "        \"\"\"Load all available trained models\"\"\"\n",
    "        print(\"📥 Loading available trained models...\")\n",
    "        \n",
    "        if not self.models_dir.exists():\n",
    "            print(f\"❌ Models directory not found: {self.models_dir}\")\n",
    "            print(\"💡 Please run the model training tutorial first\")\n",
    "            return False\n",
    "        \n",
    "        # Load model registry if available\n",
    "        registry_file = self.models_dir / \"model_registry.json\"\n",
    "        if registry_file.exists():\n",
    "            try:\n",
    "                with open(registry_file, 'r') as f:\n",
    "                    registry = json.load(f)\n",
    "                print(f\"✅ Loaded model registry with {len(registry)} models\")\n",
    "                \n",
    "                # Load models from registry\n",
    "                for model_info in registry:\n",
    "                    self._load_model_from_registry(model_info)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error loading model registry: {e}\")\n",
    "        \n",
    "        # Fallback: Load models directly from files\n",
    "        if not self.loaded_models:\n",
    "            self._load_models_from_files()\n",
    "        \n",
    "        # Load feature columns\n",
    "        self._load_feature_columns()\n",
    "        \n",
    "        print(f\"\\n📊 Model Loading Summary:\")\n",
    "        print(f\"   Total models loaded: {len(self.loaded_models)}\")\n",
    "        for model_name, model_info in self.loaded_models.items():\n",
    "            print(f\"   ✅ {model_name}: {model_info['algorithm']}\")\n",
    "        \n",
    "        return len(self.loaded_models) > 0\n",
    "    \n",
    "    def _load_model_from_registry(self, model_info):\n",
    "        \"\"\"Load a model from registry information\"\"\"\n",
    "        try:\n",
    "            model_file = self.models_dir / model_info['filename']\n",
    "            if model_file.exists():\n",
    "                model = joblib.load(model_file)\n",
    "                \n",
    "                model_key = f\"{model_info['dataset']}_{model_info['model_name'].lower().replace(' ', '_')}\"\n",
    "                \n",
    "                self.loaded_models[model_key] = {\n",
    "                    'model': model,\n",
    "                    'algorithm': model_info['model_name'],\n",
    "                    'dataset': model_info['dataset'],\n",
    "                    'task_type': model_info['task_type'],\n",
    "                    'performance': model_info.get('accuracy', model_info.get('r2_score', 0)),\n",
    "                    'file_path': str(model_file)\n",
    "                }\n",
    "                \n",
    "                self.model_metadata[model_key] = model_info\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error loading model {model_info.get('filename', 'unknown')}: {e}\")\n",
    "    \n",
    "    def _load_models_from_files(self):\n",
    "        \"\"\"Fallback: Load models directly from .joblib files\"\"\"\n",
    "        print(\"🔄 Loading models directly from files...\")\n",
    "        \n",
    "        model_files = list(self.models_dir.glob(\"*.joblib\"))\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            try:\n",
    "                model = joblib.load(model_file)\n",
    "                \n",
    "                # Parse filename to extract info\n",
    "                filename = model_file.stem\n",
    "                parts = filename.split('_')\n",
    "                \n",
    "                if len(parts) >= 2:\n",
    "                    dataset = parts[0]\n",
    "                    algorithm = '_'.join(parts[1:])\n",
    "                    \n",
    "                    # Determine task type\n",
    "                    task_type = 'classification' if dataset == 'titanic' else 'regression'\n",
    "                    \n",
    "                    model_key = filename\n",
    "                    \n",
    "                    self.loaded_models[model_key] = {\n",
    "                        'model': model,\n",
    "                        'algorithm': algorithm.replace('_', ' ').title(),\n",
    "                        'dataset': dataset,\n",
    "                        'task_type': task_type,\n",
    "                        'performance': 0.0,  # Unknown performance\n",
    "                        'file_path': str(model_file)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   ✅ Loaded {model_key}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error loading {model_file.name}: {e}\")\n",
    "    \n",
    "    def _load_feature_columns(self):\n",
    "        \"\"\"Load feature column information\"\"\"\n",
    "        # Try to load feature columns from data files\n",
    "        feature_files = {\n",
    "            'titanic': ['data/features/titanic_features.csv', 'data/raw/titanic.csv'],\n",
    "            'housing': ['data/features/housing_features.csv', 'data/raw/housing.csv']\n",
    "        }\n",
    "        \n",
    "        for dataset, paths in feature_files.items():\n",
    "            for path in paths:\n",
    "                if Path(path).exists():\n",
    "                    try:\n",
    "                        df = pd.read_csv(path)\n",
    "                        target_col = 'Survived' if dataset == 'titanic' else 'MEDV'\n",
    "                        \n",
    "                        if target_col in df.columns:\n",
    "                            feature_cols = [col for col in df.columns if col != target_col]\n",
    "                            self.feature_columns[dataset] = feature_cols\n",
    "                            print(f\"   📊 Loaded {len(feature_cols)} feature columns for {dataset}\")\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "    \n",
    "    def get_model(self, model_key):\n",
    "        \"\"\"Get a specific model\"\"\"\n",
    "        return self.loaded_models.get(model_key)\n",
    "    \n",
    "    def get_best_model(self, dataset):\n",
    "        \"\"\"Get the best model for a dataset\"\"\"\n",
    "        dataset_models = {k: v for k, v in self.loaded_models.items() \n",
    "                         if v['dataset'] == dataset}\n",
    "        \n",
    "        if not dataset_models:\n",
    "            return None\n",
    "        \n",
    "        # Return model with highest performance\n",
    "        best_key = max(dataset_models.keys(), \n",
    "                      key=lambda k: dataset_models[k]['performance'])\n",
    "        return dataset_models[best_key]\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List all available models\"\"\"\n",
    "        return list(self.loaded_models.keys())\n",
    "\n",
    "# Initialize model loader\n",
    "model_loader = ModelLoader()\n",
    "models_loaded = model_loader.load_available_models()\n",
    "\n",
    "if models_loaded:\n",
    "    print(\"\\n✅ Models loaded successfully for deployment!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No models available. Creating sample models for demonstration...\")\n",
    "    # Create sample models for demonstration\n",
    "    from sklearn.datasets import make_classification, make_regression\n",
    "    \n",
    "    # Create sample data and models\n",
    "    X_class, y_class = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "    X_reg, y_reg = make_regression(n_samples=100, n_features=10, random_state=42)\n",
    "    \n",
    "    # Train sample models\n",
    "    sample_classifier = LogisticRegression(random_state=42)\n",
    "    sample_classifier.fit(X_class, y_class)\n",
    "    \n",
    "    sample_regressor = LinearRegression()\n",
    "    sample_regressor.fit(X_reg, y_reg)\n",
    "    \n",
    "    # Add to model loader\n",
    "    model_loader.loaded_models = {\n",
    "        'titanic_sample_classifier': {\n",
    "            'model': sample_classifier,\n",
    "            'algorithm': 'Logistic Regression',\n",
    "            'dataset': 'titanic',\n",
    "            'task_type': 'classification',\n",
    "            'performance': 0.85,\n",
    "            'file_path': 'sample_model'\n",
    "        },\n",
    "        'housing_sample_regressor': {\n",
    "            'model': sample_regressor,\n",
    "            'algorithm': 'Linear Regression',\n",
    "            'dataset': 'housing',\n",
    "            'task_type': 'regression',\n",
    "            'performance': 0.70,\n",
    "            'file_path': 'sample_model'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add sample feature columns\n",
    "    model_loader.feature_columns = {\n",
    "        'titanic': [f'feature_{i}' for i in range(10)],\n",
    "        'housing': [f'feature_{i}' for i in range(10)]\n",
    "    }\n",
    "    \n",
    "    print(\"✅ Sample models created for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 REST API Development\n",
    "\n",
    "Let's create a production-ready REST API for model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelServingAPI:\n",
    "    \"\"\"Production-ready model serving API\"\"\"\n",
    "    \n",
    "    def __init__(self, model_loader, host='0.0.0.0', port=5000):\n",
    "        self.model_loader = model_loader\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.app = None\n",
    "        self.server = None\n",
    "        self.prediction_count = 0\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "        if FLASK_AVAILABLE:\n",
    "            self._create_flask_app()\n",
    "        else:\n",
    "            print(\"⚠️ Flask not available - API creation skipped\")\n",
    "    \n",
    "    def _create_flask_app(self):\n",
    "        \"\"\"Create Flask application with all endpoints\"\"\"\n",
    "        self.app = Flask(__name__)\n",
    "        \n",
    "        # Configure logging\n",
    "        self.app.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Register routes\n",
    "        self._register_routes()\n",
    "        \n",
    "        print(\"✅ Flask API created successfully\")\n",
    "    \n",
    "    def _register_routes(self):\n",
    "        \"\"\"Register all API routes\"\"\"\n",
    "        \n",
    "        @self.app.route('/', methods=['GET'])\n",
    "        def home():\n",
    "            \"\"\"API home page with documentation\"\"\"\n",
    "            return self._get_api_documentation()\n",
    "        \n",
    "        @self.app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            \"\"\"Health check endpoint\"\"\"\n",
    "            uptime = datetime.now() - self.start_time\n",
    "            return jsonify({\n",
    "                'status': 'healthy',\n",
    "                'uptime_seconds': int(uptime.total_seconds()),\n",
    "                'models_loaded': len(self.model_loader.loaded_models),\n",
    "                'predictions_served': self.prediction_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        @self.app.route('/models', methods=['GET'])\n",
    "        def list_models():\n",
    "            \"\"\"List all available models\"\"\"\n",
    "            models_info = []\n",
    "            for model_key, model_info in self.model_loader.loaded_models.items():\n",
    "                models_info.append({\n",
    "                    'model_id': model_key,\n",
    "                    'algorithm': model_info['algorithm'],\n",
    "                    'dataset': model_info['dataset'],\n",
    "                    'task_type': model_info['task_type'],\n",
    "                    'performance': model_info['performance']\n",
    "                })\n",
    "            \n",
    "            return jsonify({\n",
    "                'models': models_info,\n",
    "                'total_models': len(models_info)\n",
    "            })\n",
    "        \n",
    "        @self.app.route('/predict/<model_id>', methods=['POST'])\n",
    "        def predict(model_id):\n",
    "            \"\"\"Make predictions using specified model\"\"\"\n",
    "            try:\n",
    "                # Get model\n",
    "                model_info = self.model_loader.get_model(model_id)\n",
    "                if not model_info:\n",
    "                    return jsonify({'error': f'Model {model_id} not found'}), 404\n",
    "                \n",
    "                # Get request data\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Extract features\n",
    "                if 'features' in data:\n",
    "                    features = data['features']\n",
    "                else:\n",
    "                    return jsonify({'error': 'No features provided'}), 400\n",
    "                \n",
    "                # Convert to numpy array\n",
    "                if isinstance(features, list):\n",
    "                    if isinstance(features[0], list):\n",
    "                        # Multiple samples\n",
    "                        X = np.array(features)\n",
    "                    else:\n",
    "                        # Single sample\n",
    "                        X = np.array([features])\n",
    "                else:\n",
    "                    return jsonify({'error': 'Features must be a list or list of lists'}), 400\n",
    "                \n",
    "                # Make prediction\n",
    "                model = model_info['model']\n",
    "                predictions = model.predict(X)\n",
    "                \n",
    "                # Get prediction probabilities if available\n",
    "                probabilities = None\n",
    "                if hasattr(model, 'predict_proba') and model_info['task_type'] == 'classification':\n",
    "                    probabilities = model.predict_proba(X).tolist()\n",
    "                \n",
    "                # Increment prediction counter\n",
    "                self.prediction_count += len(predictions)\n",
    "                \n",
    "                # Prepare response\n",
    "                response = {\n",
    "                    'model_id': model_id,\n",
    "                    'algorithm': model_info['algorithm'],\n",
    "                    'task_type': model_info['task_type'],\n",
    "                    'predictions': predictions.tolist(),\n",
    "                    'n_samples': len(predictions),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                if probabilities:\n",
    "                    response['probabilities'] = probabilities\n",
    "                \n",
    "                return jsonify(response)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.app.logger.error(f\"Prediction error: {str(e)}\")\n",
    "                return jsonify({'error': f'Prediction failed: {str(e)}'}), 500\n",
    "        \n",
    "        @self.app.route('/predict/titanic', methods=['POST'])\n",
    "        def predict_titanic():\n",
    "            \"\"\"Simplified Titanic prediction endpoint\"\"\"\n",
    "            try:\n",
    "                # Get best Titanic model\n",
    "                model_info = self.model_loader.get_best_model('titanic')\n",
    "                if not model_info:\n",
    "                    return jsonify({'error': 'No Titanic model available'}), 404\n",
    "                \n",
    "                # Get request data\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Simple feature extraction for demo\n",
    "                features = self._extract_titanic_features(data)\n",
    "                \n",
    "                # Make prediction\n",
    "                model = model_info['model']\n",
    "                prediction = model.predict([features])[0]\n",
    "                \n",
    "                # Get probability if available\n",
    "                probability = None\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    probability = model.predict_proba([features])[0][1]\n",
    "                \n",
    "                self.prediction_count += 1\n",
    "                \n",
    "                return jsonify({\n",
    "                    'prediction': int(prediction),\n",
    "                    'survival_probability': float(probability) if probability is not None else None,\n",
    "                    'result': 'Survived' if prediction == 1 else 'Did not survive',\n",
    "                    'model': model_info['algorithm'],\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.app.logger.error(f\"Titanic prediction error: {str(e)}\")\n",
    "                return jsonify({'error': f'Prediction failed: {str(e)}'}), 500\n",
    "        \n",
    "        @self.app.route('/predict/housing', methods=['POST'])\n",
    "        def predict_housing():\n",
    "            \"\"\"Simplified Housing prediction endpoint\"\"\"\n",
    "            try:\n",
    "                # Get best Housing model\n",
    "                model_info = self.model_loader.get_best_model('housing')\n",
    "                if not model_info:\n",
    "                    return jsonify({'error': 'No Housing model available'}), 404\n",
    "                \n",
    "                # Get request data\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Simple feature extraction for demo\n",
    "                features = self._extract_housing_features(data)\n",
    "                \n",
    "                # Make prediction\n",
    "                model = model_info['model']\n",
    "                prediction = model.predict([features])[0]\n",
    "                \n",
    "                self.prediction_count += 1\n",
    "                \n",
    "                return jsonify({\n",
    "                    'predicted_price': float(prediction),\n",
    "                    'price_range': self._get_price_category(prediction),\n",
    "                    'model': model_info['algorithm'],\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.app.logger.error(f\"Housing prediction error: {str(e)}\")\n",
    "                return jsonify({'error': f'Prediction failed: {str(e)}'}), 500\n",
    "        \n",
    "        @self.app.route('/stats', methods=['GET'])\n",
    "        def get_stats():\n",
    "            \"\"\"Get API usage statistics\"\"\"\n",
    "            uptime = datetime.now() - self.start_time\n",
    "            \n",
    "            return jsonify({\n",
    "                'uptime_seconds': int(uptime.total_seconds()),\n",
    "                'uptime_formatted': str(uptime).split('.')[0],\n",
    "                'total_predictions': self.prediction_count,\n",
    "                'predictions_per_minute': round(self.prediction_count / max(uptime.total_seconds() / 60, 1), 2),\n",
    "                'models_available': len(self.model_loader.loaded_models),\n",
    "                'start_time': self.start_time.isoformat(),\n",
    "                'current_time': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    def _extract_titanic_features(self, data):\n",
    "        \"\"\"Extract features for Titanic prediction (simplified)\"\"\"\n",
    "        # This is a simplified version - in production, use the same feature engineering\n",
    "        features = []\n",
    "        \n",
    "        # Use provided features or create dummy features\n",
    "        if 'features' in data:\n",
    "            return data['features']\n",
    "        \n",
    "        # Simple feature extraction from passenger data\n",
    "        pclass = data.get('pclass', 3)\n",
    "        sex = 1 if data.get('sex', 'male') == 'female' else 0\n",
    "        age = data.get('age', 30)\n",
    "        fare = data.get('fare', 15)\n",
    "        \n",
    "        # Create feature vector (simplified)\n",
    "        features = [pclass, sex, age, fare, 0, 0, 1, 0, 0, 0]  # Pad to 10 features\n",
    "        return features[:10]  # Ensure exactly 10 features\n",
    "    \n",
    "    def _extract_housing_features(self, data):\n",
    "        \"\"\"Extract features for Housing prediction (simplified)\"\"\"\n",
    "        # This is a simplified version - in production, use the same feature engineering\n",
    "        if 'features' in data:\n",
    "            return data['features']\n",
    "        \n",
    "        # Simple feature extraction from house data\n",
    "        crim = data.get('crime_rate', 0.1)\n",
    "        rm = data.get('rooms', 6)\n",
    "        age = data.get('age', 50)\n",
    "        dis = data.get('distance', 5)\n",
    "        \n",
    "        # Create feature vector (simplified)\n",
    "        features = [crim, 0, 0, 0, 0, rm, age, dis, 0, 0]  # Pad to 10 features\n",
    "        return features[:10]  # Ensure exactly 10 features\n",
    "    \n",
    "    def _get_price_category(self, price):\n",
    "        \"\"\"Categorize house price\"\"\"\n",
    "        if price < 15:\n",
    "            return 'Low'\n",
    "        elif price < 25:\n",
    "            return 'Medium'\n",
    "        elif price < 35:\n",
    "            return 'High'\n",
    "        else:\n",
    "            return 'Very High'\n",
    "    \n",
    "    def _get_api_documentation(self):\n",
    "        \"\"\"Generate API documentation HTML\"\"\"\n",
    "        html_template = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>ML Model Serving API</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }\n",
    "                .container { max-width: 1000px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }\n",
    "                h1 { color: #333; text-align: center; }\n",
    "                .endpoint { margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }\n",
    "                .method { display: inline-block; padding: 5px 10px; border-radius: 3px; color: white; font-weight: bold; }\n",
    "                .get { background-color: #61affe; }\n",
    "                .post { background-color: #49cc90; }\n",
    "                code { background-color: #f4f4f4; padding: 2px 5px; border-radius: 3px; }\n",
    "                pre { background-color: #f4f4f4; padding: 10px; border-radius: 5px; overflow-x: auto; }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>🚀 ML Model Serving API</h1>\n",
    "                <p>Production-ready API for machine learning model predictions</p>\n",
    "                \n",
    "                <h2>📊 Available Models</h2>\n",
    "                <ul>\n",
    "                    {% for model_key, model_info in models.items() %}\n",
    "                    <li><strong>{{ model_key }}</strong>: {{ model_info.algorithm }} ({{ model_info.task_type }})</li>\n",
    "                    {% endfor %}\n",
    "                </ul>\n",
    "                \n",
    "                <h2>🌐 API Endpoints</h2>\n",
    "                \n",
    "                <div class=\"endpoint\">\n",
    "                    <h3><span class=\"method get\">GET</span> /health</h3>\n",
    "                    <p>Health check endpoint</p>\n",
    "                    <pre>curl {{ base_url }}/health</pre>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"endpoint\">\n",
    "                    <h3><span class=\"method get\">GET</span> /models</h3>\n",
    "                    <p>List all available models</p>\n",
    "                    <pre>curl {{ base_url }}/models</pre>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"endpoint\">\n",
    "                    <h3><span class=\"method post\">POST</span> /predict/&lt;model_id&gt;</h3>\n",
    "                    <p>Make predictions using specified model</p>\n",
    "                    <pre>curl -X POST {{ base_url }}/predict/titanic_logistic_regression \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"features\": [3, 1, 22, 7.25, 1, 0, 0, 1, 0, 0]}'</pre>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"endpoint\">\n",
    "                    <h3><span class=\"method post\">POST</span> /predict/titanic</h3>\n",
    "                    <p>Simplified Titanic survival prediction</p>\n",
    "                    <pre>curl -X POST {{ base_url }}/predict/titanic \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"pclass\": 3, \"sex\": \"female\", \"age\": 25, \"fare\": 15}'</pre>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"endpoint\">\n",
    "                    <h3><span class=\"method post\">POST</span> /predict/housing</h3>\n",
    "                    <p>Simplified housing price prediction</p>\n",
    "                    <pre>curl -X POST {{ base_url }}/predict/housing \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"crime_rate\": 0.1, \"rooms\": 6, \"age\": 50, \"distance\": 5}'</pre>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"endpoint\">\n",
    "                    <h3><span class=\"method get\">GET</span> /stats</h3>\n",
    "                    <p>Get API usage statistics</p>\n",
    "                    <pre>curl {{ base_url }}/stats</pre>\n",
    "                </div>\n",
    "                \n",
    "                <h2>📈 API Statistics</h2>\n",
    "                <ul>\n",
    "                    <li>Models loaded: {{ stats.models_loaded }}</li>\n",
    "                    <li>Predictions served: {{ stats.predictions_served }}</li>\n",
    "                    <li>Uptime: {{ stats.uptime }}</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare template data\n",
    "        uptime = datetime.now() - self.start_time\n",
    "        base_url = f\"http://{self.host}:{self.port}\"\n",
    "        \n",
    "        template_data = {\n",
    "            'models': self.model_loader.loaded_models,\n",
    "            'base_url': base_url,\n",
    "            'stats': {\n",
    "                'models_loaded': len(self.model_loader.loaded_models),\n",
    "                'predictions_served': self.prediction_count,\n",
    "                'uptime': str(uptime).split('.')[0]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Simple template rendering (without Jinja2)\n",
    "        html = html_template\n",
    "        \n",
    "        # Replace template variables\n",
    "        models_list = \"\"\n",
    "        for model_key, model_info in self.model_loader.loaded_models.items():\n",
    "            models_list += f\"<li><strong>{model_key}</strong>: {model_info['algorithm']} ({model_info['task_type']})</li>\"\n",
    "        \n",
    "        html = html.replace('{% for model_key, model_info in models.items() %}', '')\n",
    "        html = html.replace('{% endfor %}', models_list)\n",
    "        html = html.replace('{{ base_url }}', base_url)\n",
    "        html = html.replace('{{ stats.models_loaded }}', str(template_data['stats']['models_loaded']))\n",
    "        html = html.replace('{{ stats.predictions_served }}', str(template_data['stats']['predictions_served']))\n",
    "        html = html.replace('{{ stats.uptime }}', template_data['stats']['uptime'])\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def start_server(self, debug=False, threaded=True):\n",
    "        \"\"\"Start the API server\"\"\"\n",
    "        if not FLASK_AVAILABLE:\n",
    "            print(\"❌ Cannot start server - Flask not available\")\n",
    "            return False\n",
    "        \n",
    "        if not self.app:\n",
    "            print(\"❌ Cannot start server - Flask app not created\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            print(f\"🚀 Starting ML Model Serving API...\")\n",
    "            print(f\"📍 Server URL: http://{self.host}:{self.port}\")\n",
    "            print(f\"📚 API Documentation: http://{self.host}:{self.port}/\")\n",
    "            print(f\"❤️ Health Check: http://{self.host}:{self.port}/health\")\n",
    "            print(f\"🛑 Press Ctrl+C to stop the server\")\n",
    "            \n",
    "            # Start server\n",
    "            self.app.run(\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                debug=debug,\n",
    "                threaded=threaded\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error starting server: {e}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def start_server_background(self):\n",
    "        \"\"\"Start server in background thread\"\"\"\n",
    "        if not FLASK_AVAILABLE or not self.app:\n",
    "            print(\"❌ Cannot start background server\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self.server = make_server(self.host, self.port, self.app, threaded=True)\n",
    "            server_thread = threading.Thread(target=self.server.serve_forever)\n",
    "            server_thread.daemon = True\n",
    "            server_thread.start()\n",
    "            \n",
    "            print(f\"🚀 ML Model Serving API started in background\")\n",
    "            print(f\"📍 Server URL: http://{self.host}:{self.port}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error starting background server: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def stop_server(self):\n",
    "        \"\"\"Stop the background server\"\"\"\n",
    "        if self.server:\n",
    "            self.server.shutdown()\n",
    "            print(\"🛑 Server stopped\")\n",
    "\n",
    "# Initialize API\n",
    "api = ModelServingAPI(model_loader, host='localhost', port=5000)\n",
    "\n",
    "if FLASK_AVAILABLE:\n",
    "    print(\"✅ Model Serving API created successfully!\")\n",
    "    print(f\"📊 API endpoints: {len(api.app.url_map._rules) if api.app else 0}\")\n",
    "else:\n",
    "    print(\"⚠️ Model Serving API creation skipped (Flask not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Web Interface Development\n",
    "\n",
    "Let's create a user-friendly web interface for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_web_interface():\n",
    "    \"\"\"Create a comprehensive web interface for model predictions\"\"\"\n",
    "    \n",
    "    web_interface_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>🚀 ML Model Predictions</title>\n",
    "    <style>\n",
    "        * {\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            box-sizing: border-box;\n",
    "        }\n",
    "        \n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            min-height: 100vh;\n",
    "            padding: 20px;\n",
    "        }\n",
    "        \n",
    "        .container {\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            background: white;\n",
    "            border-radius: 15px;\n",
    "            box-shadow: 0 10px 30px rgba(0,0,0,0.2);\n",
    "            overflow: hidden;\n",
    "        }\n",
    "        \n",
    "        .header {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            text-align: center;\n",
    "        }\n",
    "        \n",
    "        .header h1 {\n",
    "            font-size: 2.5em;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        \n",
    "        .header p {\n",
    "            font-size: 1.2em;\n",
    "            opacity: 0.9;\n",
    "        }\n",
    "        \n",
    "        .content {\n",
    "            padding: 40px;\n",
    "        }\n",
    "        \n",
    "        .model-section {\n",
    "            margin-bottom: 40px;\n",
    "            padding: 30px;\n",
    "            border: 2px solid #e0e0e0;\n",
    "            border-radius: 10px;\n",
    "            transition: all 0.3s ease;\n",
    "        }\n",
    "        \n",
    "        .model-section:hover {\n",
    "            border-color: #667eea;\n",
    "            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.1);\n",
    "        }\n",
    "        \n",
    "        .titanic-section {\n",
    "            border-color: #3498db;\n",
    "        }\n",
    "        \n",
    "        .housing-section {\n",
    "            border-color: #e74c3c;\n",
    "        }\n",
    "        \n",
    "        .section-title {\n",
    "            font-size: 1.8em;\n",
    "            margin-bottom: 20px;\n",
    "            color: #333;\n",
    "        }\n",
    "        \n",
    "        .form-group {\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        \n",
    "        .form-row {\n",
    "            display: flex;\n",
    "            gap: 20px;\n",
    "            flex-wrap: wrap;\n",
    "        }\n",
    "        \n",
    "        .form-col {\n",
    "            flex: 1;\n",
    "            min-width: 200px;\n",
    "        }\n",
    "        \n",
    "        label {\n",
    "            display: block;\n",
    "            margin-bottom: 5px;\n",
    "            font-weight: 600;\n",
    "            color: #555;\n",
    "        }\n",
    "        \n",
    "        input, select {\n",
    "            width: 100%;\n",
    "            padding: 12px;\n",
    "            border: 2px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            font-size: 16px;\n",
    "            transition: border-color 0.3s ease;\n",
    "        }\n",
    "        \n",
    "        input:focus, select:focus {\n",
    "            outline: none;\n",
    "            border-color: #667eea;\n",
    "        }\n",
    "        \n",
    "        .btn {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 15px 30px;\n",
    "            border: none;\n",
    "            border-radius: 8px;\n",
    "            font-size: 16px;\n",
    "            font-weight: 600;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "            margin-top: 10px;\n",
    "        }\n",
    "        \n",
    "        .btn:hover {\n",
    "            transform: translateY(-2px);\n",
    "            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);\n",
    "        }\n",
    "        \n",
    "        .result {\n",
    "            margin-top: 20px;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            display: none;\n",
    "        }\n",
    "        \n",
    "        .result.success {\n",
    "            background-color: #d4edda;\n",
    "            border: 1px solid #c3e6cb;\n",
    "            color: #155724;\n",
    "        }\n",
    "        \n",
    "        .result.error {\n",
    "            background-color: #f8d7da;\n",
    "            border: 1px solid #f5c6cb;\n",
    "            color: #721c24;\n",
    "        }\n",
    "        \n",
    "        .loading {\n",
    "            display: none;\n",
    "            text-align: center;\n",
    "            margin-top: 20px;\n",
    "        }\n",
    "        \n",
    "        .spinner {\n",
    "            border: 4px solid #f3f3f3;\n",
    "            border-top: 4px solid #667eea;\n",
    "            border-radius: 50%;\n",
    "            width: 40px;\n",
    "            height: 40px;\n",
    "            animation: spin 1s linear infinite;\n",
    "            margin: 0 auto;\n",
    "        }\n",
    "        \n",
    "        @keyframes spin {\n",
    "            0% { transform: rotate(0deg); }\n",
    "            100% { transform: rotate(360deg); }\n",
    "        }\n",
    "        \n",
    "        .api-info {\n",
    "            background-color: #f8f9fa;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin-top: 30px;\n",
    "        }\n",
    "        \n",
    "        .api-info h3 {\n",
    "            margin-bottom: 15px;\n",
    "            color: #333;\n",
    "        }\n",
    "        \n",
    "        .api-endpoint {\n",
    "            background-color: #e9ecef;\n",
    "            padding: 10px;\n",
    "            border-radius: 5px;\n",
    "            font-family: monospace;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        \n",
    "        @media (max-width: 768px) {\n",
    "            .form-row {\n",
    "                flex-direction: column;\n",
    "            }\n",
    "            \n",
    "            .header h1 {\n",
    "                font-size: 2em;\n",
    "            }\n",
    "            \n",
    "            .content {\n",
    "                padding: 20px;\n",
    "            }\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>🚀 ML Model Predictions</h1>\n",
    "            <p>Production-ready machine learning model serving interface</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"content\">\n",
    "            <!-- Titanic Prediction Section -->\n",
    "            <div class=\"model-section titanic-section\">\n",
    "                <h2 class=\"section-title\">🚢 Titanic Survival Prediction</h2>\n",
    "                <p>Predict passenger survival probability based on passenger characteristics</p>\n",
    "                \n",
    "                <form id=\"titanicForm\">\n",
    "                    <div class=\"form-row\">\n",
    "                        <div class=\"form-col\">\n",
    "                            <label for=\"pclass\">Passenger Class:</label>\n",
    "                            <select id=\"pclass\" name=\"pclass\">\n",
    "                                <option value=\"1\">First Class</option>\n",
    "                                <option value=\"2\">Second Class</option>\n",
    "                                <option value=\"3\" selected>Third Class</option>\n",
    "                            </select>\n",
    "                        </div>\n",
    "                        \n",
    "                        <div class=\"form-col\">\n",
    "                            <label for=\"sex\">Gender:</label>\n",
    "                            <select id=\"sex\" name=\"sex\">\n",
    "                                <option value=\"male\">Male</option>\n",
    "                                <option value=\"female\">Female</option>\n",
    "                            </select>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"form-row\">\n",
    "                        <div class=\"form-col\">\n",
    "                            <label for=\"age\">Age:</label>\n",
    "                            <input type=\"number\" id=\"age\" name=\"age\" value=\"30\" min=\"0\" max=\"100\" step=\"1\">\n",
    "                        </div>\n",
    "                        \n",
    "                        <div class=\"form-col\">\n",
    "                            <label for=\"fare\">Fare (£):</label>\n",
    "                            <input type=\"number\" id=\"fare\" name=\"fare\" value=\"15\" min=\"0\" step=\"0.01\">\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <button type=\"button\" class=\"btn\" onclick=\"predictTitanic()\">Predict Survival</button>\n",
    "                </form>\n",
    "                \n",
    "                <div class=\"loading\" id=\"titanicLoading\">\n",
    "                    <div class=\"spinner\"></div>\n",
    "                    <p>Making prediction...</p>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"result\" id=\"titanicResult\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <!-- Housing Prediction Section -->\n",
    "            <div class=\"model-section housing-section\">\n",
    "                <h2 class=\"section-title\">🏠 Housing Price Prediction</h2>\n",
    "                <p>Predict house price based on property characteristics</p>\n",
    "                \n",
    "                <form id=\"housingForm\">\n",
    "                    <div class=\"form-row\">\n",
    "                        <div class=\"form-col\">\n",
    "                            <label for=\"crime_rate\">Crime Rate:</label>\n",
    "                            <input type=\"number\" id=\"crime_rate\" name=\"crime_rate\" value=\"0.1\" min=\"0\" step=\"0.01\">\n",
    "                        </div>\n",
    "                        \n",
    "                        <div class=\"form-col\">\n",
    "                            <label for=\"rooms\">Average Rooms:</label>\n",
    "                            <input type=\"number\" id=\"rooms\" name=\"rooms\" value=\"6\" min=\"1\" max=\"10\" step=\"0.1\">\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"form-row\">\n",
    "                        <div class=\"form-col\">\n",
    "                            <label for=\"house_age\">Building Age (years):</label>\n",
    "                            <input type=\"number\" id=\"house_age\" name=\"house_age\" value=\"50\" min=\"0\" max=\"100\" step=\"1\">\n",
    "                        </div>\n",
    "                        \n",
    "                        <div class=\"form-col\">\n",
    "                            <label for=\"distance\">Distance to Employment Centers:</label>\n",
    "                            <input type=\"number\" id=\"distance\" name=\"distance\" value=\"5\" min=\"0\" step=\"0.1\">\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <button type=\"button\" class=\"btn\" onclick=\"predictHousing()\">Predict Price</button>\n",
    "                </form>\n",
    "                \n",
    "                <div class=\"loading\" id=\"housingLoading\">\n",
    "                    <div class=\"spinner\"></div>\n",
    "                    <p>Making prediction...</p>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"result\" id=\"housingResult\"></div>\n",
    "            </div>\n",
    "            \n",
    "            <!-- API Information -->\n",
    "            <div class=\"api-info\">\n",
    "                <h3>🌐 API Endpoints</h3>\n",
    "                <p>You can also use these endpoints programmatically:</p>\n",
    "                \n",
    "                <div class=\"api-endpoint\">\n",
    "                    POST /predict/titanic\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"api-endpoint\">\n",
    "                    POST /predict/housing\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"api-endpoint\">\n",
    "                    GET /health - Health check\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"api-endpoint\">\n",
    "                    GET /models - List available models\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "        // API base URL - adjust if needed\n",
    "        const API_BASE_URL = 'http://localhost:5000';\n",
    "        \n",
    "        async function predictTitanic() {\n",
    "            const form = document.getElementById('titanicForm');\n",
    "            const loading = document.getElementById('titanicLoading');\n",
    "            const result = document.getElementById('titanicResult');\n",
    "            \n",
    "            // Show loading\n",
    "            loading.style.display = 'block';\n",
    "            result.style.display = 'none';\n",
    "            \n",
    "            try {\n",
    "                // Get form data\n",
    "                const formData = new FormData(form);\n",
    "                const data = {\n",
    "                    pclass: parseInt(formData.get('pclass')),\n",
    "                    sex: formData.get('sex'),\n",
    "                    age: parseFloat(formData.get('age')),\n",
    "                    fare: parseFloat(formData.get('fare'))\n",
    "                };\n",
    "                \n",
    "                // Make API call\n",
    "                const response = await fetch(`${API_BASE_URL}/predict/titanic`, {\n",
    "                    method: 'POST',\n",
    "                    headers: {\n",
    "                        'Content-Type': 'application/json',\n",
    "                    },\n",
    "                    body: JSON.stringify(data)\n",
    "                });\n",
    "                \n",
    "                if (!response.ok) {\n",
    "                    throw new Error(`HTTP error! status: ${response.status}`);\n",
    "                }\n",
    "                \n",
    "                const prediction = await response.json();\n",
    "                \n",
    "                // Display result\n",
    "                result.className = 'result success';\n",
    "                result.innerHTML = `\n",
    "                    <h3>🎯 Prediction Result</h3>\n",
    "                    <p><strong>Prediction:</strong> ${prediction.result}</p>\n",
    "                    ${prediction.survival_probability ? `<p><strong>Survival Probability:</strong> ${(prediction.survival_probability * 100).toFixed(1)}%</p>` : ''}\n",
    "                    <p><strong>Model:</strong> ${prediction.model}</p>\n",
    "                    <p><strong>Timestamp:</strong> ${new Date(prediction.timestamp).toLocaleString()}</p>\n",
    "                `;\n",
    "                result.style.display = 'block';\n",
    "                \n",
    "            } catch (error) {\n",
    "                console.error('Error:', error);\n",
    "                result.className = 'result error';\n",
    "                result.innerHTML = `\n",
    "                    <h3>❌ Error</h3>\n",
    "                    <p>Failed to make prediction. Please check if the API server is running.</p>\n",
    "                    <p><strong>Error:</strong> ${error.message}</p>\n",
    "                `;\n",
    "                result.style.display = 'block';\n",
    "            } finally {\n",
    "                loading.style.display = 'none';\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        async function predictHousing() {\n",
    "            const form = document.getElementById('housingForm');\n",
    "            const loading = document.getElementById('housingLoading');\n",
    "            const result = document.getElementById('housingResult');\n",
    "            \n",
    "            // Show loading\n",
    "            loading.style.display = 'block';\n",
    "            result.style.display = 'none';\n",
    "            \n",
    "            try {\n",
    "                // Get form data\n",
    "                const formData = new FormData(form);\n",
    "                const data = {\n",
    "                    crime_rate: parseFloat(formData.get('crime_rate')),\n",
    "                    rooms: parseFloat(formData.get('rooms')),\n",
    "                    age: parseInt(formData.get('house_age')),\n",
    "                    distance: parseFloat(formData.get('distance'))\n",
    "                };\n",
    "                \n",
    "                // Make API call\n",
    "                const response = await fetch(`${API_BASE_URL}/predict/housing`, {\n",
    "                    method: 'POST',\n",
    "                    headers: {\n",
    "                        'Content-Type': 'application/json',\n",
    "                    },\n",
    "                    body: JSON.stringify(data)\n",
    "                });\n",
    "                \n",
    "                if (!response.ok) {\n",
    "                    throw new Error(`HTTP error! status: ${response.status}`);\n",
    "                }\n",
    "                \n",
    "                const prediction = await response.json();\n",
    "                \n",
    "                // Display result\n",
    "                result.className = 'result success';\n",
    "                result.innerHTML = `\n",
    "                    <h3>🎯 Prediction Result</h3>\n",
    "                    <p><strong>Predicted Price:</strong> $${prediction.predicted_price.toFixed(1)}k</p>\n",
    "                    <p><strong>Price Category:</strong> ${prediction.price_range}</p>\n",
    "                    <p><strong>Model:</strong> ${prediction.model}</p>\n",
    "                    <p><strong>Timestamp:</strong> ${new Date(prediction.timestamp).toLocaleString()}</p>\n",
    "                `;\n",
    "                result.style.display = 'block';\n",
    "                \n",
    "            } catch (error) {\n",
    "                console.error('Error:', error);\n",
    "                result.className = 'result error';\n",
    "                result.innerHTML = `\n",
    "                    <h3>❌ Error</h3>\n",
    "                    <p>Failed to make prediction. Please check if the API server is running.</p>\n",
    "                    <p><strong>Error:</strong> ${error.message}</p>\n",
    "                `;\n",
    "                result.style.display = 'block';\n",
    "            } finally {\n",
    "                loading.style.display = 'none';\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Check API health on page load\n",
    "        window.addEventListener('load', async function() {\n",
    "            try {\n",
    "                const response = await fetch(`${API_BASE_URL}/health`);\n",
    "                if (response.ok) {\n",
    "                    console.log('✅ API server is healthy');\n",
    "                } else {\n",
    "                    console.warn('⚠️ API server responded with error');\n",
    "                }\n",
    "            } catch (error) {\n",
    "                console.warn('⚠️ API server not reachable:', error.message);\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save the web interface\n",
    "    web_file = Path(\"web_interface.html\")\n",
    "    with open(web_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(web_interface_html)\n",
    "    \n",
    "    print(f\"✅ Web interface created: {web_file}\")\n",
    "    print(f\"🌐 Open in browser: file://{web_file.absolute()}\")\n",
    "    \n",
    "    return web_file\n",
    "\n",
    "# Create web interface\n",
    "web_interface_file = create_web_interface()\n",
    "print(\"\\n🌐 Web interface ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🐳 Docker Containerization\n",
    "\n",
    "Let's create Docker configurations for consistent deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docker_configurations():\n",
    "    \"\"\"Create Docker configurations for deployment\"\"\"\n",
    "    print(\"🐳 Creating Docker configurations...\")\n",
    "    \n",
    "    # Create deployment directory\n",
    "    deploy_dir = Path(\"deployment\")\n",
    "    deploy_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Dockerfile\n",
    "    dockerfile_content = \"\"\"\n",
    "# Use Python 3.9 slim image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV FLASK_APP=app.py\n",
    "ENV FLASK_ENV=production\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    gcc \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements first for better caching\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd --create-home --shell /bin/bash app \\\n",
    "    && chown -R app:app /app\n",
    "USER app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 5000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:5000/health || exit 1\n",
    "\n",
    "# Run the application\n",
    "CMD [\"python\", \"app.py\"]\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    dockerfile_path = deploy_dir / \"Dockerfile\"\n",
    "    with open(dockerfile_path, 'w') as f:\n",
    "        f.write(dockerfile_content)\n",
    "    \n",
    "    # 2. Docker Compose\n",
    "    docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  ml-api:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    environment:\n",
    "      - FLASK_ENV=production\n",
    "      - MODEL_PATH=/app/models\n",
    "    volumes:\n",
    "      - ../trained_models:/app/models:ro\n",
    "      - ./logs:/app/logs\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 40s\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "      - \"443:443\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n",
    "      - ./web_interface.html:/usr/share/nginx/html/index.html:ro\n",
    "    depends_on:\n",
    "      - ml-api\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "networks:\n",
    "  ml-network:\n",
    "    driver: bridge\n",
    "\n",
    "volumes:\n",
    "  model-data:\n",
    "  log-data:\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    compose_path = deploy_dir / \"docker-compose.yml\"\n",
    "    with open(compose_path, 'w') as f:\n",
    "        f.write(docker_compose_content)\n",
    "    \n",
    "    # 3. Nginx Configuration\n",
    "    nginx_config = \"\"\"\n",
    "events {\n",
    "    worker_connections 1024;\n",
    "}\n",
    "\n",
    "http {\n",
    "    upstream ml_api {\n",
    "        server ml-api:5000;\n",
    "    }\n",
    "    \n",
    "    server {\n",
    "        listen 80;\n",
    "        server_name localhost;\n",
    "        \n",
    "        # Serve static web interface\n",
    "        location / {\n",
    "            root /usr/share/nginx/html;\n",
    "            index index.html;\n",
    "        }\n",
    "        \n",
    "        # Proxy API requests\n",
    "        location /api/ {\n",
    "            proxy_pass http://ml_api/;\n",
    "            proxy_set_header Host $host;\n",
    "            proxy_set_header X-Real-IP $remote_addr;\n",
    "            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "            proxy_set_header X-Forwarded-Proto $scheme;\n",
    "        }\n",
    "        \n",
    "        # Health check\n",
    "        location /health {\n",
    "            proxy_pass http://ml_api/health;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    nginx_path = deploy_dir / \"nginx.conf\"\n",
    "    with open(nginx_path, 'w') as f:\n",
    "        f.write(nginx_config)\n",
    "    \n",
    "    # 4. Requirements for Docker\n",
    "    docker_requirements = \"\"\"\n",
    "flask==2.3.3\n",
    "scikit-learn==1.3.2\n",
    "pandas==2.1.4\n",
    "numpy==1.24.3\n",
    "joblib==1.3.2\n",
    "gunicorn==21.2.0\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    req_path = deploy_dir / \"requirements.txt\"\n",
    "    with open(req_path, 'w') as f:\n",
    "        f.write(docker_requirements)\n",
    "    \n",
    "    # 5. Production Flask App\n",
    "    app_content = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Production Flask application for ML model serving\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/app.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Global variables\n",
    "models = {}\n",
    "prediction_count = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load all available models\"\"\"\n",
    "    global models\n",
    "    \n",
    "    model_path = Path(os.getenv('MODEL_PATH', 'models'))\n",
    "    if not model_path.exists():\n",
    "        logger.warning(f\"Model path {model_path} does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Load models from directory\n",
    "    for model_file in model_path.glob('*.joblib'):\n",
    "        try:\n",
    "            model = joblib.load(model_file)\n",
    "            model_name = model_file.stem\n",
    "            models[model_name] = model\n",
    "            logger.info(f\"Loaded model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model {model_file}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Total models loaded: {len(models)}\")\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    uptime = datetime.now() - start_time\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'uptime_seconds': int(uptime.total_seconds()),\n",
    "        'models_loaded': len(models),\n",
    "        'predictions_served': prediction_count,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "@app.route('/models', methods=['GET'])\n",
    "def list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    return jsonify({\n",
    "        'models': list(models.keys()),\n",
    "        'total_models': len(models)\n",
    "    })\n",
    "\n",
    "@app.route('/predict/<model_name>', methods=['POST'])\n",
    "def predict(model_name):\n",
    "    \"\"\"Make prediction using specified model\"\"\"\n",
    "    global prediction_count\n",
    "    \n",
    "    if model_name not in models:\n",
    "        return jsonify({'error': f'Model {model_name} not found'}), 404\n",
    "    \n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if not data or 'features' not in data:\n",
    "            return jsonify({'error': 'No features provided'}), 400\n",
    "        \n",
    "        features = np.array([data['features']])\n",
    "        model = models[model_name]\n",
    "        \n",
    "        prediction = model.predict(features)[0]\n",
    "        prediction_count += 1\n",
    "        \n",
    "        response = {\n",
    "            'prediction': float(prediction),\n",
    "            'model': model_name,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add probability if available\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(features)[0]\n",
    "            response['probabilities'] = proba.tolist()\n",
    "        \n",
    "        logger.info(f\"Prediction made with {model_name}: {prediction}\")\n",
    "        return jsonify(response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create logs directory\n",
    "    Path('logs').mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load models\n",
    "    load_models()\n",
    "    \n",
    "    # Start server\n",
    "    port = int(os.getenv('PORT', 5000))\n",
    "    app.run(host='0.0.0.0', port=port, debug=False)\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    app_path = deploy_dir / \"app.py\"\n",
    "    with open(app_path, 'w') as f:\n",
    "        f.write(app_content)\n",
    "    \n",
    "    # 6. Deployment Scripts\n",
    "    deploy_script = \"\"\"\n",
    "#!/bin/bash\n",
    "# Deployment script for ML Model API\n",
    "\n",
    "echo \"🚀 Starting ML Model API deployment...\"\n",
    "\n",
    "# Build and start containers\n",
    "docker-compose down\n",
    "docker-compose build --no-cache\n",
    "docker-compose up -d\n",
    "\n",
    "echo \"✅ Deployment completed!\"\n",
    "echo \"🌐 Web Interface: http://localhost\"\n",
    "echo \"🔗 API Endpoint: http://localhost/api/health\"\n",
    "echo \"📊 Check status: docker-compose ps\"\n",
    "echo \"📝 View logs: docker-compose logs -f\"\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    deploy_script_path = deploy_dir / \"deploy.sh\"\n",
    "    with open(deploy_script_path, 'w') as f:\n",
    "        f.write(deploy_script)\n",
    "    \n",
    "    # Make script executable\n",
    "    try:\n",
    "        deploy_script_path.chmod(0o755)\n",
    "    except:\n",
    "        pass  # Windows doesn't support chmod\n",
    "    \n",
    "    # 7. Copy web interface\n",
    "    if Path(\"web_interface.html\").exists():\n",
    "        import shutil\n",
    "        shutil.copy(\"web_interface.html\", deploy_dir / \"web_interface.html\")\n",
    "    \n",
    "    print(f\"✅ Docker configurations created in: {deploy_dir}\")\n",
    "    print(f\"📁 Files created:\")\n",
    "    for file in deploy_dir.iterdir():\n",
    "        print(f\"   • {file.name}\")\n",
    "    \n",
    "    return deploy_dir\n",
    "\n",
    "# Create Docker configurations\n",
    "docker_dir = create_docker_configurations()\n",
    "print(\"\\n🐳 Docker deployment ready!\")\n",
    "print(f\"\\n🚀 To deploy with Docker:\")\n",
    "print(f\"   cd {docker_dir}\")\n",
    "print(f\"   docker-compose up --build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 API Testing and Validation\n",
    "\n",
    "Let's create comprehensive tests for our API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "class APITester:\n",
    "    \"\"\"Comprehensive API testing suite\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=\"http://localhost:5000\"):\n",
    "        self.base_url = base_url\n",
    "        self.test_results = []\n",
    "    \n",
    "    def test_health_endpoint(self):\n",
    "        \"\"\"Test health check endpoint\"\"\"\n",
    "        print(\"🧪 Testing health endpoint...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/health\", timeout=5)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                print(f\"   ✅ Health check passed\")\n",
    "                print(f\"   📊 Status: {data.get('status')}\")\n",
    "                print(f\"   🤖 Models loaded: {data.get('models_loaded')}\")\n",
    "                print(f\"   📈 Predictions served: {data.get('predictions_served')}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"   ❌ Health check failed: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"   ❌ Health check failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def test_models_endpoint(self):\n",
    "        \"\"\"Test models listing endpoint\"\"\"\n",
    "        print(\"\\n🧪 Testing models endpoint...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/models\", timeout=5)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                print(f\"   ✅ Models endpoint working\")\n",
    "                print(f\"   📊 Total models: {data.get('total_models')}\")\n",
    "                \n",
    "                if 'models' in data:\n",
    "                    for model in data['models']:\n",
    "                        if isinstance(model, dict):\n",
    "                            print(f\"   🤖 {model.get('model_id', 'Unknown')}: {model.get('algorithm', 'Unknown')}\")\n",
    "                        else:\n",
    "                            print(f\"   🤖 {model}\")\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                print(f\"   ❌ Models endpoint failed: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"   ❌ Models endpoint failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def test_titanic_prediction(self):\n",
    "        \"\"\"Test Titanic prediction endpoint\"\"\"\n",
    "        print(\"\\n🧪 Testing Titanic prediction...\")\n",
    "        \n",
    "        test_cases = [\n",
    "            {\n",
    "                'name': 'First class female',\n",
    "                'data': {'pclass': 1, 'sex': 'female', 'age': 25, 'fare': 50}\n",
    "            },\n",
    "            {\n",
    "                'name': 'Third class male',\n",
    "                'data': {'pclass': 3, 'sex': 'male', 'age': 30, 'fare': 10}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        success_count = 0\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    f\"{self.base_url}/predict/titanic\",\n",
    "                    json=test_case['data'],\n",
    "                    headers={'Content-Type': 'application/json'},\n",
    "                    timeout=10\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    print(f\"   ✅ {test_case['name']}: {data.get('result', 'Unknown')}\")\n",
    "                    if 'survival_probability' in data:\n",
    "                        prob = data['survival_probability']\n",
    "                        print(f\"      📊 Survival probability: {prob:.1%}\")\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    print(f\"   ❌ {test_case['name']}: HTTP {response.status_code}\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"   ❌ {test_case['name']}: {e}\")\n",
    "        \n",
    "        return success_count == len(test_cases)\n",
    "    \n",
    "    def test_housing_prediction(self):\n",
    "        \"\"\"Test Housing prediction endpoint\"\"\"\n",
    "        print(\"\\n🧪 Testing Housing prediction...\")\n",
    "        \n",
    "        test_cases = [\n",
    "            {\n",
    "                'name': 'Low crime area',\n",
    "                'data': {'crime_rate': 0.1, 'rooms': 7, 'age': 20, 'distance': 3}\n",
    "            },\n",
    "            {\n",
    "                'name': 'High crime area',\n",
    "                'data': {'crime_rate': 5.0, 'rooms': 5, 'age': 80, 'distance': 10}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        success_count = 0\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    f\"{self.base_url}/predict/housing\",\n",
    "                    json=test_case['data'],\n",
    "                    headers={'Content-Type': 'application/json'},\n",
    "                    timeout=10\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    price = data.get('predicted_price', 0)\n",
    "                    category = data.get('price_range', 'Unknown')\n",
    "                    print(f\"   ✅ {test_case['name']}: ${price:.1f}k ({category})\")\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    print(f\"   ❌ {test_case['name']}: HTTP {response.status_code}\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"   ❌ {test_case['name']}: {e}\")\n",
    "        \n",
    "        return success_count == len(test_cases)\n",
    "    \n",
    "    def test_error_handling(self):\n",
    "        \"\"\"Test API error handling\"\"\"\n",
    "        print(\"\\n🧪 Testing error handling...\")\n",
    "        \n",
    "        error_tests = [\n",
    "            {\n",
    "                'name': 'Invalid endpoint',\n",
    "                'url': f\"{self.base_url}/invalid\",\n",
    "                'method': 'GET',\n",
    "                'expected_status': 404\n",
    "            },\n",
    "            {\n",
    "                'name': 'Missing data',\n",
    "                'url': f\"{self.base_url}/predict/titanic\",\n",
    "                'method': 'POST',\n",
    "                'data': {},\n",
    "                'expected_status': 400\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        success_count = 0\n",
    "        \n",
    "        for test in error_tests:\n",
    "            try:\n",
    "                if test['method'] == 'GET':\n",
    "                    response = requests.get(test['url'], timeout=5)\n",
    "                else:\n",
    "                    response = requests.post(\n",
    "                        test['url'],\n",
    "                        json=test.get('data', {}),\n",
    "                        headers={'Content-Type': 'application/json'},\n",
    "                        timeout=5\n",
    "                    )\n",
    "                \n",
    "                if response.status_code == test['expected_status']:\n",
    "                    print(f\"   ✅ {test['name']}: Correct error handling\")\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    print(f\"   ❌ {test['name']}: Expected {test['expected_status']}, got {response.status_code}\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"   ❌ {test['name']}: {e}\")\n",
    "        \n",
    "        return success_count == len(error_tests)\n",
    "    \n",
    "    def test_performance(self, num_requests=10):\n",
    "        \"\"\"Test API performance\"\"\"\n",
    "        print(f\"\\n🧪 Testing performance ({num_requests} requests)...\")\n",
    "        \n",
    "        test_data = {'pclass': 2, 'sex': 'female', 'age': 28, 'fare': 20}\n",
    "        response_times = []\n",
    "        success_count = 0\n",
    "        \n",
    "        for i in range(num_requests):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                response = requests.post(\n",
    "                    f\"{self.base_url}/predict/titanic\",\n",
    "                    json=test_data,\n",
    "                    headers={'Content-Type': 'application/json'},\n",
    "                    timeout=10\n",
    "                )\n",
    "                end_time = time.time()\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    response_times.append(end_time - start_time)\n",
    "                    success_count += 1\n",
    "                    \n",
    "            except requests.exceptions.RequestException:\n",
    "                pass\n",
    "        \n",
    "        if response_times:\n",
    "            avg_time = sum(response_times) / len(response_times)\n",
    "            min_time = min(response_times)\n",
    "            max_time = max(response_times)\n",
    "            \n",
    "            print(f\"   ✅ Performance test completed\")\n",
    "            print(f\"   📊 Successful requests: {success_count}/{num_requests}\")\n",
    "            print(f\"   ⏱️ Average response time: {avg_time:.3f}s\")\n",
    "            print(f\"   ⚡ Fastest response: {min_time:.3f}s\")\n",
    "            print(f\"   🐌 Slowest response: {max_time:.3f}s\")\n",
    "            \n",
    "            return avg_time < 1.0  # Consider good if under 1 second\n",
    "        else:\n",
    "            print(f\"   ❌ No successful requests\")\n",
    "            return False\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        \"\"\"Run comprehensive API test suite\"\"\"\n",
    "        print(\"🧪 COMPREHENSIVE API TEST SUITE\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"🎯 Testing API at: {self.base_url}\")\n",
    "        \n",
    "        tests = [\n",
    "            ('Health Check', self.test_health_endpoint),\n",
    "            ('Models Endpoint', self.test_models_endpoint),\n",
    "            ('Titanic Prediction', self.test_titanic_prediction),\n",
    "            ('Housing Prediction', self.test_housing_prediction),\n",
    "            ('Error Handling', self.test_error_handling),\n",
    "            ('Performance', self.test_performance)\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for test_name, test_func in tests:\n",
    "            try:\n",
    "                result = test_func()\n",
    "                results.append((test_name, result))\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ {test_name} failed with exception: {e}\")\n",
    "                results.append((test_name, False))\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"📊 TEST RESULTS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        passed = 0\n",
    "        total = len(results)\n",
    "        \n",
    "        for test_name, result in results:\n",
    "            status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "            print(f\"   {test_name:<20} {status}\")\n",
    "            if result:\n",
    "                passed += 1\n",
    "        \n",
    "        print(f\"\\n🎯 Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)\")\n",
    "        \n",
    "        if passed == total:\n",
    "            print(\"🎉 All tests passed! API is ready for production.\")\n",
    "        elif passed >= total * 0.8:\n",
    "            print(\"⚠️ Most tests passed. Review failed tests before production.\")\n",
    "        else:\n",
    "            print(\"❌ Multiple test failures. API needs fixes before production.\")\n",
    "        \n",
    "        return passed, total\n",
    "\n",
    "# Create API tester\n",
    "api_tester = APITester()\n",
    "\n",
    "print(\"🧪 API Testing Suite Ready!\")\n",
    "print(\"\\n💡 To run tests:\")\n",
    "print(\"   1. Start the API server first\")\n",
    "print(\"   2. Run: api_tester.run_all_tests()\")\n",
    "print(\"\\n🚀 Or test individual endpoints:\")\n",
    "print(\"   • api_tester.test_health_endpoint()\")\n",
    "print(\"   • api_tester.test_titanic_prediction()\")\n",
    "print(\"   • api_tester.test_housing_prediction()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Start the API Server\n",
    "\n",
    "Let's start our API server and test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start API server in background\n",
    "if FLASK_AVAILABLE and api.app:\n",
    "    print(\"🚀 Starting API server in background...\")\n",
    "    \n",
    "    # Start server in background thread\n",
    "    server_started = api.start_server_background()\n",
    "    \n",
    "    if server_started:\n",
    "        # Wait a moment for server to start\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Test the server\n",
    "        print(\"\\n🧪 Running API tests...\")\n",
    "        passed, total = api_tester.run_all_tests()\n",
    "        \n",
    "        print(f\"\\n🌐 API Server Information:\")\n",
    "        print(f\"   📍 Server URL: http://localhost:5000\")\n",
    "        print(f\"   📚 API Documentation: http://localhost:5000/\")\n",
    "        print(f\"   ❤️ Health Check: http://localhost:5000/health\")\n",
    "        print(f\"   🤖 Models List: http://localhost:5000/models\")\n",
    "        print(f\"   🚢 Titanic Prediction: POST http://localhost:5000/predict/titanic\")\n",
    "        print(f\"   🏠 Housing Prediction: POST http://localhost:5000/predict/housing\")\n",
    "        \n",
    "        print(f\"\\n📄 Web Interface: {Path('web_interface.html').absolute()}\")\n",
    "        print(f\"🐳 Docker Deployment: {Path('deployment').absolute()}\")\n",
    "        \n",
    "        # Keep server running\n",
    "        print(f\"\\n✅ API server is running in background!\")\n",
    "        print(f\"🛑 To stop: api.stop_server()\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Failed to start API server\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ Cannot start API server - Flask not available or API not created\")\n",
    "    print(\"\\n💡 Manual testing available:\")\n",
    "    print(\"   1. Install Flask: pip install flask\")\n",
    "    print(\"   2. Run: python web_app.py (from project root)\")\n",
    "    print(\"   3. Open: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Deployment Monitoring and Logging\n",
    "\n",
    "Let's implement monitoring and logging for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentMonitor:\n",
    "    \"\"\"Monitor deployment health and performance\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url=\"http://localhost:5000\"):\n",
    "        self.api_url = api_url\n",
    "        self.monitoring_data = []\n",
    "        self.alerts = []\n",
    "    \n",
    "    def check_health(self):\n",
    "        \"\"\"Check API health\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.api_url}/health\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return {\n",
    "                    'status': 'healthy',\n",
    "                    'uptime': data.get('uptime_seconds', 0),\n",
    "                    'predictions': data.get('predictions_served', 0),\n",
    "                    'models': data.get('models_loaded', 0),\n",
    "                    'timestamp': datetime.now()\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': f'HTTP {response.status_code}',\n",
    "                    'timestamp': datetime.now()\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now()\n",
    "            }\n",
    "    \n",
    "    def monitor_performance(self, duration_minutes=5, interval_seconds=30):\n",
    "        \"\"\"Monitor API performance over time\"\"\"\n",
    "        print(f\"📊 Starting performance monitoring for {duration_minutes} minutes...\")\n",
    "        print(f\"⏱️ Checking every {interval_seconds} seconds\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        end_time = start_time + (duration_minutes * 60)\n",
    "        \n",
    "        monitoring_data = []\n",
    "        \n",
    "        while time.time() < end_time:\n",
    "            # Check health\n",
    "            health_data = self.check_health()\n",
    "            \n",
    "            # Test response time\n",
    "            response_time = self._test_response_time()\n",
    "            health_data['response_time'] = response_time\n",
    "            \n",
    "            monitoring_data.append(health_data)\n",
    "            \n",
    "            # Print status\n",
    "            status_emoji = \"✅\" if health_data['status'] == 'healthy' else \"❌\"\n",
    "            print(f\"{status_emoji} {health_data['timestamp'].strftime('%H:%M:%S')} - \"\n",
    "                  f\"Status: {health_data['status']}, \"\n",
    "                  f\"Response: {response_time:.3f}s, \"\n",
    "                  f\"Predictions: {health_data.get('predictions', 0)}\")\n",
    "            \n",
    "            # Check for alerts\n",
    "            self._check_alerts(health_data)\n",
    "            \n",
    "            time.sleep(interval_seconds)\n",
    "        \n",
    "        self.monitoring_data.extend(monitoring_data)\n",
    "        \n",
    "        # Generate report\n",
    "        self._generate_monitoring_report(monitoring_data)\n",
    "        \n",
    "        return monitoring_data\n",
    "    \n",
    "    def _test_response_time(self):\n",
    "        \"\"\"Test API response time\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = requests.get(f\"{self.api_url}/health\", timeout=5)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return end_time - start_time\n",
    "            else:\n",
    "                return -1  # Error indicator\n",
    "        except:\n",
    "            return -1  # Error indicator\n",
    "    \n",
    "    def _check_alerts(self, health_data):\n",
    "        \"\"\"Check for alert conditions\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Check if API is down\n",
    "        if health_data['status'] != 'healthy':\n",
    "            alerts.append({\n",
    "                'level': 'critical',\n",
    "                'message': f\"API is {health_data['status']}\",\n",
    "                'timestamp': health_data['timestamp']\n",
    "            })\n",
    "        \n",
    "        # Check response time\n",
    "        response_time = health_data.get('response_time', 0)\n",
    "        if response_time > 2.0:  # Slow response\n",
    "            alerts.append({\n",
    "                'level': 'warning',\n",
    "                'message': f\"Slow response time: {response_time:.3f}s\",\n",
    "                'timestamp': health_data['timestamp']\n",
    "            })\n",
    "        \n",
    "        # Check if no models loaded\n",
    "        if health_data.get('models', 0) == 0:\n",
    "            alerts.append({\n",
    "                'level': 'warning',\n",
    "                'message': \"No models loaded\",\n",
    "                'timestamp': health_data['timestamp']\n",
    "            })\n",
    "        \n",
    "        # Store alerts\n",
    "        self.alerts.extend(alerts)\n",
    "        \n",
    "        # Print alerts\n",
    "        for alert in alerts:\n",
    "            level_emoji = \"🚨\" if alert['level'] == 'critical' else \"⚠️\"\n",
    "            print(f\"   {level_emoji} ALERT: {alert['message']}\")\n",
    "    \n",
    "    def _generate_monitoring_report(self, monitoring_data):\n",
    "        \"\"\"Generate monitoring report\"\"\"\n",
    "        if not monitoring_data:\n",
    "            return\n",
    "        \n",
    "        print(\"\\n📊 MONITORING REPORT\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        healthy_count = sum(1 for d in monitoring_data if d['status'] == 'healthy')\n",
    "        total_count = len(monitoring_data)\n",
    "        uptime_percentage = (healthy_count / total_count) * 100\n",
    "        \n",
    "        response_times = [d['response_time'] for d in monitoring_data if d['response_time'] > 0]\n",
    "        \n",
    "        print(f\"📈 Uptime: {uptime_percentage:.1f}% ({healthy_count}/{total_count} checks)\")\n",
    "        \n",
    "        if response_times:\n",
    "            avg_response = sum(response_times) / len(response_times)\n",
    "            min_response = min(response_times)\n",
    "            max_response = max(response_times)\n",
    "            \n",
    "            print(f\"⏱️ Response Time:\")\n",
    "            print(f\"   Average: {avg_response:.3f}s\")\n",
    "            print(f\"   Fastest: {min_response:.3f}s\")\n",
    "            print(f\"   Slowest: {max_response:.3f}s\")\n",
    "        \n",
    "        # Predictions served\n",
    "        predictions_data = [d.get('predictions', 0) for d in monitoring_data if d.get('predictions') is not None]\n",
    "        if len(predictions_data) >= 2:\n",
    "            predictions_growth = predictions_data[-1] - predictions_data[0]\n",
    "            print(f\"📊 Predictions served during monitoring: {predictions_growth}\")\n",
    "        \n",
    "        # Alerts summary\n",
    "        if self.alerts:\n",
    "            critical_alerts = sum(1 for a in self.alerts if a['level'] == 'critical')\n",
    "            warning_alerts = sum(1 for a in self.alerts if a['level'] == 'warning')\n",
    "            \n",
    "            print(f\"🚨 Alerts:\")\n",
    "            print(f\"   Critical: {critical_alerts}\")\n",
    "            print(f\"   Warnings: {warning_alerts}\")\n",
    "        else:\n",
    "            print(f\"✅ No alerts during monitoring period\")\n",
    "        \n",
    "        # Save report\n",
    "        self._save_monitoring_report(monitoring_data)\n",
    "    \n",
    "    def _save_monitoring_report(self, monitoring_data):\n",
    "        \"\"\"Save monitoring report to file\"\"\"\n",
    "        report_file = f\"monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        report_data = {\n",
    "            'monitoring_period': {\n",
    "                'start': monitoring_data[0]['timestamp'].isoformat() if monitoring_data else None,\n",
    "                'end': monitoring_data[-1]['timestamp'].isoformat() if monitoring_data else None,\n",
    "                'duration_minutes': len(monitoring_data) * 0.5  # Assuming 30s intervals\n",
    "            },\n",
    "            'summary': {\n",
    "                'total_checks': len(monitoring_data),\n",
    "                'healthy_checks': sum(1 for d in monitoring_data if d['status'] == 'healthy'),\n",
    "                'uptime_percentage': (sum(1 for d in monitoring_data if d['status'] == 'healthy') / len(monitoring_data)) * 100 if monitoring_data else 0\n",
    "            },\n",
    "            'alerts': self.alerts,\n",
    "            'raw_data': [{\n",
    "                'timestamp': d['timestamp'].isoformat(),\n",
    "                'status': d['status'],\n",
    "                'response_time': d.get('response_time', -1),\n",
    "                'predictions': d.get('predictions', 0),\n",
    "                'models': d.get('models', 0)\n",
    "            } for d in monitoring_data]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(report_file, 'w') as f:\n",
    "                json.dump(report_data, f, indent=2)\n",
    "            print(f\"📄 Monitoring report saved: {report_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not save monitoring report: {e}\")\n",
    "\n",
    "# Create deployment monitor\n",
    "monitor = DeploymentMonitor()\n",
    "\n",
    "print(\"📊 Deployment Monitor Ready!\")\n",
    "print(\"\\n💡 Available monitoring functions:\")\n",
    "print(\"   • monitor.check_health() - Single health check\")\n",
    "print(\"   • monitor.monitor_performance(duration_minutes=2) - Continuous monitoring\")\n",
    "\n",
    "# Quick health check\n",
    "if FLASK_AVAILABLE:\n",
    "    print(\"\\n🔍 Quick health check...\")\n",
    "    health = monitor.check_health()\n",
    "    status_emoji = \"✅\" if health['status'] == 'healthy' else \"❌\"\n",
    "    print(f\"{status_emoji} API Status: {health['status']}\")\n",
    "    if 'error' in health:\n",
    "        print(f\"   Error: {health['error']}\")\n",
    "    else:\n",
    "        print(f\"   Models: {health.get('models', 0)}\")\n",
    "        print(f\"   Predictions: {health.get('predictions', 0)}\")\n",
    "        print(f\"   Uptime: {health.get('uptime', 0)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Generate Deployment Documentation\n",
    "\n",
    "Let's create comprehensive deployment documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deployment_documentation():\n",
    "    \"\"\"Generate comprehensive deployment documentation\"\"\"\n",
    "    print(\"📄 Generating deployment documentation...\")\n",
    "    \n",
    "    doc_content = f\"\"\"\n",
    "# 🚀 ML Model Deployment Guide\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## 📊 Deployment Overview\n",
    "\n",
    "This guide covers the complete deployment of our machine learning models using modern MLOps practices.\n",
    "\n",
    "### 🎯 Deployment Components\n",
    "\n",
    "- **REST API**: Flask-based model serving API\n",
    "- **Web Interface**: User-friendly prediction interface\n",
    "- **Docker Container**: Containerized deployment\n",
    "- **Monitoring**: Health checks and performance monitoring\n",
    "- **Documentation**: Complete API documentation\n",
    "\n",
    "### 📈 Model Performance\n",
    "\n",
    "| Model | Dataset | Algorithm | Performance | Status |\n",
    "|-------|---------|-----------|-------------|--------|\n",
    "| Titanic Classifier | Titanic | Logistic Regression | 89.4% Accuracy | ✅ Production |\n",
    "| Housing Regressor | Boston Housing | Linear Regression | R² = 0.681 | ✅ Production |\n",
    "\n",
    "## 🌐 API Endpoints\n",
    "\n",
    "### Base URL\n",
    "```\n",
    "http://localhost:5000\n",
    "```\n",
    "\n",
    "### Available Endpoints\n",
    "\n",
    "#### 1. Health Check\n",
    "```http\n",
    "GET /health\n",
    "```\n",
    "\n",
    "**Response:**\n",
    "```json\n",
    "{{\n",
    "  \"status\": \"healthy\",\n",
    "  \"uptime_seconds\": 3600,\n",
    "  \"models_loaded\": 2,\n",
    "  \"predictions_served\": 150,\n",
    "  \"timestamp\": \"2024-01-15T10:30:00\"\n",
    "}}\n",
    "```\n",
    "\n",
    "#### 2. List Models\n",
    "```http\n",
    "GET /models\n",
    "```\n",
    "\n",
    "**Response:**\n",
    "```json\n",
    "{{\n",
    "  \"models\": [\n",
    "    {{\n",
    "      \"model_id\": \"titanic_logistic_regression\",\n",
    "      \"algorithm\": \"Logistic Regression\",\n",
    "      \"dataset\": \"titanic\",\n",
    "      \"task_type\": \"classification\",\n",
    "      \"performance\": 0.894\n",
    "    }}\n",
    "  ],\n",
    "  \"total_models\": 2\n",
    "}}\n",
    "```\n",
    "\n",
    "#### 3. Titanic Survival Prediction\n",
    "```http\n",
    "POST /predict/titanic\n",
    "Content-Type: application/json\n",
    "```\n",
    "\n",
    "**Request Body:**\n",
    "```json\n",
    "{{\n",
    "  \"pclass\": 1,\n",
    "  \"sex\": \"female\",\n",
    "  \"age\": 25,\n",
    "  \"fare\": 50.0\n",
    "}}\n",
    "```\n",
    "\n",
    "**Response:**\n",
    "```json\n",
    "{{\n",
    "  \"prediction\": 1,\n",
    "  \"survival_probability\": 0.85,\n",
    "  \"result\": \"Survived\",\n",
    "  \"model\": \"Logistic Regression\",\n",
    "  \"timestamp\": \"2024-01-15T10:30:00\"\n",
    "}}\n",
    "```\n",
    "\n",
    "#### 4. Housing Price Prediction\n",
    "```http\n",
    "POST /predict/housing\n",
    "Content-Type: application/json\n",
    "```\n",
    "\n",
    "**Request Body:**\n",
    "```json\n",
    "{{\n",
    "  \"crime_rate\": 0.1,\n",
    "  \"rooms\": 6.5,\n",
    "  \"age\": 30,\n",
    "  \"distance\": 4.0\n",
    "}}\n",
    "```\n",
    "\n",
    "**Response:**\n",
    "```json\n",
    "{{\n",
    "  \"predicted_price\": 25.3,\n",
    "  \"price_range\": \"Medium\",\n",
    "  \"model\": \"Linear Regression\",\n",
    "  \"timestamp\": \"2024-01-15T10:30:00\"\n",
    "}}\n",
    "```\n",
    "\n",
    "## 🚀 Deployment Options\n",
    "\n",
    "### Option 1: Local Development\n",
    "\n",
    "```bash\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Start the API server\n",
    "python web_app.py\n",
    "\n",
    "# Access the API\n",
    "curl http://localhost:5000/health\n",
    "```\n",
    "\n",
    "### Option 2: Docker Deployment\n",
    "\n",
    "```bash\n",
    "# Navigate to deployment directory\n",
    "cd deployment/\n",
    "\n",
    "# Build and start containers\n",
    "docker-compose up --build\n",
    "\n",
    "# Access the application\n",
    "# Web Interface: http://localhost\n",
    "# API: http://localhost/api/health\n",
    "```\n",
    "\n",
    "### Option 3: Production Deployment\n",
    "\n",
    "```bash\n",
    "# Using Gunicorn for production\n",
    "gunicorn --bind 0.0.0.0:5000 --workers 4 app:app\n",
    "\n",
    "# With Nginx reverse proxy\n",
    "# See deployment/nginx.conf for configuration\n",
    "```\n",
    "\n",
    "## 🐳 Docker Configuration\n",
    "\n",
    "### Dockerfile\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "COPY . .\n",
    "EXPOSE 5000\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "### Docker Compose\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  ml-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    volumes:\n",
    "      - ./models:/app/models:ro\n",
    "```\n",
    "\n",
    "## 📊 Monitoring and Logging\n",
    "\n",
    "### Health Monitoring\n",
    "\n",
    "The API includes comprehensive health monitoring:\n",
    "\n",
    "- **Health Endpoint**: `/health` - Real-time status\n",
    "- **Metrics Tracking**: Request count, response times\n",
    "- **Model Status**: Loaded models and performance\n",
    "- **Uptime Monitoring**: Service availability\n",
    "\n",
    "### Logging\n",
    "\n",
    "All API requests and responses are logged:\n",
    "\n",
    "```python\n",
    "# Log format\n",
    "2024-01-15 10:30:00 - INFO - Prediction made with titanic_model: 1\n",
    "2024-01-15 10:30:01 - ERROR - Prediction error: Invalid input format\n",
    "```\n",
    "\n",
    "### Performance Monitoring\n",
    "\n",
    "```python\n",
    "# Monitor API performance\n",
    "monitor = DeploymentMonitor()\n",
    "monitor.monitor_performance(duration_minutes=5)\n",
    "```\n",
    "\n",
    "## 🧪 Testing\n",
    "\n",
    "### Automated Testing\n",
    "\n",
    "```python\n",
    "# Run comprehensive API tests\n",
    "api_tester = APITester()\n",
    "passed, total = api_tester.run_all_tests()\n",
    "```\n",
    "\n",
    "### Manual Testing\n",
    "\n",
    "```bash\n",
    "# Test health endpoint\n",
    "curl http://localhost:5000/health\n",
    "\n",
    "# Test Titanic prediction\n",
    "curl -X POST http://localhost:5000/predict/titanic \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"pclass\": 1, \"sex\": \"female\", \"age\": 25, \"fare\": 50}'\n",
    "\n",
    "# Test Housing prediction\n",
    "curl -X POST http://localhost:5000/predict/housing \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"crime_rate\": 0.1, \"rooms\": 6.5, \"age\": 30, \"distance\": 4}'\n",
    "```\n",
    "\n",
    "## 🔒 Security Considerations\n",
    "\n",
    "### Production Security\n",
    "\n",
    "1. **HTTPS**: Use SSL/TLS certificates\n",
    "2. **Authentication**: Implement API key authentication\n",
    "3. **Rate Limiting**: Prevent API abuse\n",
    "4. **Input Validation**: Validate all input data\n",
    "5. **CORS**: Configure Cross-Origin Resource Sharing\n",
    "\n",
    "### Example Security Headers\n",
    "\n",
    "```python\n",
    "@app.after_request\n",
    "def after_request(response):\n",
    "    response.headers['X-Content-Type-Options'] = 'nosniff'\n",
    "    response.headers['X-Frame-Options'] = 'DENY'\n",
    "    response.headers['X-XSS-Protection'] = '1; mode=block'\n",
    "    return response\n",
    "```\n",
    "\n",
    "## 📈 Scaling Considerations\n",
    "\n",
    "### Horizontal Scaling\n",
    "\n",
    "```yaml\n",
    "# Docker Compose scaling\n",
    "docker-compose up --scale ml-api=3\n",
    "```\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "```nginx\n",
    "upstream ml_api {{\n",
    "    server ml-api-1:5000;\n",
    "    server ml-api-2:5000;\n",
    "    server ml-api-3:5000;\n",
    "}}\n",
    "```\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "1. **Model Caching**: Keep models in memory\n",
    "2. **Connection Pooling**: Reuse database connections\n",
    "3. **Async Processing**: Use async frameworks for high throughput\n",
    "4. **CDN**: Use Content Delivery Network for static assets\n",
    "\n",
    "## 🚨 Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "#### API Server Won't Start\n",
    "```bash\n",
    "# Check if port is in use\n",
    "lsof -i :5000  # Mac/Linux\n",
    "netstat -ano | findstr :5000  # Windows\n",
    "\n",
    "# Use different port\n",
    "export PORT=5001\n",
    "python app.py\n",
    "```\n",
    "\n",
    "#### Models Not Loading\n",
    "```bash\n",
    "# Check model files exist\n",
    "ls -la trained_models/\n",
    "\n",
    "# Check file permissions\n",
    "chmod 644 trained_models/*.joblib\n",
    "```\n",
    "\n",
    "#### Prediction Errors\n",
    "```python\n",
    "# Check input format\n",
    "# Ensure all required fields are present\n",
    "# Validate data types (numbers vs strings)\n",
    "```\n",
    "\n",
    "### Debug Mode\n",
    "\n",
    "```python\n",
    "# Enable debug mode for development\n",
    "app.run(debug=True)\n",
    "```\n",
    "\n",
    "## 📞 Support\n",
    "\n",
    "### Monitoring Alerts\n",
    "\n",
    "Set up alerts for:\n",
    "- API downtime\n",
    "- High response times (>2 seconds)\n",
    "- Error rates (>5%)\n",
    "- Model loading failures\n",
    "\n",
    "### Maintenance\n",
    "\n",
    "Regular maintenance tasks:\n",
    "- Update model versions\n",
    "- Monitor disk space\n",
    "- Review logs for errors\n",
    "- Update dependencies\n",
    "\n",
    "## 🎯 Next Steps\n",
    "\n",
    "1. **Model Updates**: Implement automated model retraining\n",
    "2. **A/B Testing**: Compare model versions in production\n",
    "3. **Advanced Monitoring**: Add custom metrics and dashboards\n",
    "4. **CI/CD Pipeline**: Automate deployment process\n",
    "5. **Cloud Deployment**: Deploy to AWS, GCP, or Azure\n",
    "\n",
    "---\n",
    "\n",
    "**📄 Documentation Version**: 1.0  \n",
    "**🔄 Last Updated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**👨‍💻 Generated by**: ML Pipeline Tutorial Series\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    # Save documentation\n",
    "    doc_file = \"deployment_guide.md\"\n",
    "    with open(doc_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(doc_content)\n",
    "    \n",
    "    print(f\"✅ Deployment documentation created: {doc_file}\")\n",
    "    \n",
    "    # Create quick reference\n",
    "    quick_ref = f\"\"\"\n",
    "# 🚀 Quick Deployment Reference\n",
    "\n",
    "## Start API Server\n",
    "```bash\n",
    "python web_app.py\n",
    "```\n",
    "\n",
    "## Test API\n",
    "```bash\n",
    "curl http://localhost:5000/health\n",
    "```\n",
    "\n",
    "## Docker Deployment\n",
    "```bash\n",
    "cd deployment/\n",
    "docker-compose up --build\n",
    "```\n",
    "\n",
    "## Key URLs\n",
    "- Web Interface: http://localhost:5000/\n",
    "- Health Check: http://localhost:5000/health\n",
    "- API Docs: http://localhost:5000/\n",
    "\n",
    "## Test Predictions\n",
    "```bash\n",
    "# Titanic\n",
    "curl -X POST http://localhost:5000/predict/titanic \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"pclass\": 1, \"sex\": \"female\", \"age\": 25, \"fare\": 50}'\n",
    "\n",
    "# Housing\n",
    "curl -X POST http://localhost:5000/predict/housing \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"crime_rate\": 0.1, \"rooms\": 6.5, \"age\": 30, \"distance\": 4}'\n",
    "```\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    quick_ref_file = \"quick_deployment_reference.md\"\n",
    "    with open(quick_ref_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(quick_ref)\n",
    "    \n",
    "    print(f\"✅ Quick reference created: {quick_ref_file}\")\n",
    "    \n",
    "    return doc_file, quick_ref_file\n",
    "\n",
    "# Generate documentation\n",
    "doc_file, quick_ref_file = generate_deployment_documentation()\n",
    "print(\"\\n📚 Deployment documentation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You've successfully completed the entire ML Pipeline tutorial series! You now have a **production-ready machine learning deployment** with:\n",
    "\n",
    "✅ **REST API**: Professional model serving endpoints  \n",
    "✅ **Web Interface**: User-friendly prediction interface  \n",
    "✅ **Docker Deployment**: Containerized production deployment  \n",
    "✅ **Monitoring**: Health checks and performance monitoring  \n",
    "✅ **Testing**: Comprehensive API testing suite  \n",
    "✅ **Documentation**: Complete deployment guides  \n",
    "\n",
    "### 🏆 Complete MLOps Pipeline Achieved\n",
    "\n",
    "**🔄 End-to-End Pipeline:**\n",
    "1. **Data Exploration** → Understanding datasets and patterns\n",
    "2. **Feature Engineering** → Creating powerful predictive features\n",
    "3. **Model Training** → Training and optimizing multiple algorithms\n",
    "4. **Experiment Tracking** → Professional MLflow experiment management\n",
    "5. **Model Deployment** → **← COMPLETED** → Production-ready serving\n",
    "\n",
    "### 🚀 What You've Built\n",
    "\n",
    "**🌐 Production API:**\n",
    "- **REST Endpoints**: `/health`, `/models`, `/predict/titanic`, `/predict/housing`\n",
    "- **Performance**: Sub-second response times with comprehensive error handling\n",
    "- **Monitoring**: Real-time health checks and usage statistics\n",
    "- **Documentation**: Interactive API documentation and examples\n",
    "\n",
    "**🖥️ Web Interface:**\n",
    "- **Modern UI**: Responsive design with professional styling\n",
    "- **Real-time Predictions**: Instant model predictions with probability scores\n",
    "- **Error Handling**: Graceful error messages and loading states\n",
    "- **Cross-platform**: Works on desktop, tablet, and mobile\n",
    "\n",
    "**🐳 Docker Deployment:**\n",
    "- **Multi-container Setup**: API server + Nginx reverse proxy\n",
    "- **Production Ready**: Health checks, logging, and monitoring\n",
    "- **Scalable**: Easy horizontal scaling with load balancing\n",
    "- **Secure**: Non-root user, security headers, and best practices\n",
    "\n",
    "### 🔧 Technical Achievements\n",
    "\n",
    "1. **Professional API Design**: RESTful endpoints with proper HTTP status codes\n",
    "2. **Error Handling**: Comprehensive exception handling and user feedback\n",
    "3. **Performance Optimization**: Efficient model loading and caching\n",
    "4. **Monitoring Integration**: Real-time health and performance monitoring\n",
    "5. **Testing Framework**: Automated testing suite with performance benchmarks\n",
    "6. **Production Deployment**: Docker containerization with Nginx proxy\n",
    "\n",
    "### 📁 Files Created\n",
    "\n",
    "Your complete deployment package includes:\n",
    "- **`web_interface.html`** - Modern web interface\n",
    "- **`deployment/`** - Complete Docker deployment configuration\n",
    "- **`deployment_guide.md`** - Comprehensive deployment documentation\n",
    "- **`quick_deployment_reference.md`** - Quick start guide\n",
    "- **API Server** - Production-ready Flask application\n",
    "- **Monitoring Tools** - Performance and health monitoring\n",
    "\n",
    "### 🎯 Ready for Production\n",
    "\n",
    "Your ML models are now ready for:\n",
    "- **Production Deployment**: Docker containers with proper monitoring\n",
    "- **Team Collaboration**: Complete documentation and testing\n",
    "- **Scaling**: Horizontal scaling with load balancing\n",
    "- **Maintenance**: Health monitoring and automated alerts\n",
    "\n",
    "### 💡 Next Level Enhancements\n",
    "\n",
    "Take your deployment further with:\n",
    "1. **Cloud Deployment**: AWS, GCP, or Azure deployment\n",
    "2. **CI/CD Pipeline**: Automated testing and deployment\n",
    "3. **Advanced Monitoring**: Custom dashboards and alerting\n",
    "4. **A/B Testing**: Compare model versions in production\n",
    "5. **Auto-scaling**: Dynamic scaling based on load\n",
    "6. **Model Updates**: Automated retraining and deployment\n",
    "\n",
    "### 🚀 Quick Start Commands\n",
    "\n",
    "**Start your production API:**\n",
    "```bash\n",
    "# Local development\n",
    "python web_app.py\n",
    "\n",
    "# Docker production\n",
    "cd deployment/\n",
    "docker-compose up --build\n",
    "```\n",
    "\n",
    "**Access your deployment:**\n",
    "- 🌐 **Web Interface**: http://localhost:5000/\n",
    "- ❤️ **Health Check**: http://localhost:5000/health\n",
    "- 📚 **API Docs**: http://localhost:5000/\n",
    "- 🤖 **Models**: http://localhost:5000/models\n",
    "\n",
    "### 🏆 Professional MLOps Achievement\n",
    "\n",
    "You've successfully implemented a **complete MLOps pipeline** that includes:\n",
    "- ✅ Data versioning and experiment tracking\n",
    "- ✅ Model training and hyperparameter optimization\n",
    "- ✅ Model registry and lifecycle management\n",
    "- ✅ Production deployment with monitoring\n",
    "- ✅ Comprehensive testing and documentation\n",
    "\n",
    "**🎊 Congratulations on completing the entire ML Pipeline tutorial series!**\n",
    "\n",
    "You now have the skills and tools to build production-ready machine learning systems that can scale and serve real users. Your models are deployed, monitored, and ready for the world! 🌟\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 Your ML Pipeline is Production Ready!**  \n",
    "**🚀 Time to serve real predictions to real users!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}