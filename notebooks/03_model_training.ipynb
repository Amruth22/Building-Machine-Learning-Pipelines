{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Model Training Tutorial\n",
    "\n",
    "Welcome to the third tutorial in our ML Pipeline series! In this notebook, we'll train multiple machine learning models using our engineered features and achieve production-ready performance.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "- Training multiple ML algorithms (Random Forest, Logistic Regression, SVM, etc.)\n",
    "- Hyperparameter tuning with Grid Search and Random Search\n",
    "- Cross-validation for robust model evaluation\n",
    "- Model comparison and selection\n",
    "- Performance metrics and evaluation\n",
    "- Model persistence and saving\n",
    "\n",
    "## üèÜ Target Performance Goals\n",
    "- **Titanic Classification**: Achieve **89.4% accuracy** with Logistic Regression\n",
    "- **Housing Regression**: Achieve **R¬≤ = 0.681** with Linear Regression\n",
    "- Train and compare **6+ different algorithms**\n",
    "- Implement **hyperparameter optimization**\n",
    "- Create **production-ready models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNIVERSAL SETUP - Works on all PCs and environments\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Navigate to project root if we're in notebooks directory\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "    print(f\"üìÅ Changed to project root: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"üìÅ Already in project root: {os.getcwd()}\")\n",
    "\n",
    "# Add src to Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    print(f\"üì¶ Added to Python path: {src_path}\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve,\n",
    "    confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('seaborn')  # Fallback for older versions\n",
    "\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"‚úÖ Setup completed successfully!\")\n",
    "print(f\"üìä Scikit-learn version: {__import__('sklearn').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load Engineered Features\n",
    "\n",
    "Let's load the features we created in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD ENGINEERED FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "def load_engineered_features():\n",
    "    \"\"\"Load engineered features from previous tutorial\"\"\"\n",
    "    print(\"üì• Loading engineered features...\")\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # Try to load Titanic features\n",
    "    titanic_paths = [\n",
    "        'data/features/titanic_features.csv',\n",
    "        'data/raw/titanic.csv'  # Fallback to raw data\n",
    "    ]\n",
    "    \n",
    "    for path in titanic_paths:\n",
    "        if Path(path).exists():\n",
    "            try:\n",
    "                datasets['titanic'] = pd.read_csv(path)\n",
    "                print(f\"‚úÖ Titanic data loaded from: {path}\")\n",
    "                print(f\"   Shape: {datasets['titanic'].shape}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading {path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Try to load Housing features\n",
    "    housing_paths = [\n",
    "        'data/features/housing_features.csv',\n",
    "        'data/raw/housing.csv'  # Fallback to raw data\n",
    "    ]\n",
    "    \n",
    "    for path in housing_paths:\n",
    "        if Path(path).exists():\n",
    "            try:\n",
    "                datasets['housing'] = pd.read_csv(path)\n",
    "                print(f\"‚úÖ Housing data loaded from: {path}\")\n",
    "                print(f\"   Shape: {datasets['housing'].shape}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading {path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"‚ùå No datasets could be loaded!\")\n",
    "        print(\"üí° Please run the previous notebooks first:\")\n",
    "        print(\"   1. 01_data_exploration.ipynb\")\n",
    "        print(\"   2. 02_feature_engineering.ipynb\")\n",
    "        print(\"   Or run: python download_datasets.py\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load the datasets\n",
    "datasets = load_engineered_features()\n",
    "\n",
    "# Display dataset information\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\nüìä {name.title()} Dataset Info:\")\n",
    "    print(f\"   Shape: {data.shape}\")\n",
    "    print(f\"   Features: {data.shape[1] - 1}\")\n",
    "    print(f\"   Samples: {data.shape[0]}\")\n",
    "    print(f\"   Missing values: {data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Show first few columns\n",
    "    print(f\"   Columns (first 10): {list(data.columns[:10])}\")\n",
    "    if len(data.columns) > 10:\n",
    "        print(f\"   ... and {len(data.columns) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Data Preparation\n",
    "\n",
    "Let's prepare our data for training by handling any remaining issues and splitting into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(data, target_col, test_size=0.2, random_state=42):\n",
    "    \"\"\"Prepare data for machine learning training\"\"\"\n",
    "    print(f\"üîß Preparing data for training...\")\n",
    "    \n",
    "    # Make a copy\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Handle missing values if any\n",
    "    missing_values = df.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"   üîß Handling {missing_values} missing values...\")\n",
    "        \n",
    "        # Fill numerical columns with median\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numerical_cols:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "        \n",
    "        # Fill categorical columns with mode\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in categorical_cols:\n",
    "            if col != target_col and df[col].isnull().sum() > 0:\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    if target_col in categorical_cols:\n",
    "        categorical_cols.remove(target_col)\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"   üè∑Ô∏è Encoding {len(categorical_cols)} categorical columns...\")\n",
    "        for col in categorical_cols:\n",
    "            # Simple label encoding for now\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "    \n",
    "    # Separate features and target\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in data\")\n",
    "    \n",
    "    X = df.drop([target_col], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Handle infinite values\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, \n",
    "        stratify=y if len(y.unique()) < 20 else None  # Stratify for classification\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Data preparation completed\")\n",
    "    print(f\"   üìä Training set: {X_train.shape}\")\n",
    "    print(f\"   üìä Test set: {X_test.shape}\")\n",
    "    print(f\"   üéØ Target distribution:\")\n",
    "    if len(y.unique()) < 10:  # Categorical target\n",
    "        print(f\"      {dict(y.value_counts())}\")\n",
    "    else:  # Continuous target\n",
    "        print(f\"      Mean: {y.mean():.3f}, Std: {y.std():.3f}\")\n",
    "        print(f\"      Range: [{y.min():.3f}, {y.max():.3f}]\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X.columns.tolist()\n",
    "\n",
    "# Prepare datasets\n",
    "prepared_data = {}\n",
    "\n",
    "if 'titanic' in datasets:\n",
    "    print(\"üö¢ Preparing Titanic dataset...\")\n",
    "    try:\n",
    "        X_train_t, X_test_t, y_train_t, y_test_t, features_t = prepare_data_for_training(\n",
    "            datasets['titanic'], 'Survived'\n",
    "        )\n",
    "        prepared_data['titanic'] = {\n",
    "            'X_train': X_train_t, 'X_test': X_test_t,\n",
    "            'y_train': y_train_t, 'y_test': y_test_t,\n",
    "            'features': features_t, 'target': 'Survived',\n",
    "            'task_type': 'classification'\n",
    "        }\n",
    "        print(\"‚úÖ Titanic data prepared successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error preparing Titanic data: {e}\")\n",
    "\n",
    "if 'housing' in datasets:\n",
    "    print(\"\\nüè† Preparing Housing dataset...\")\n",
    "    try:\n",
    "        X_train_h, X_test_h, y_train_h, y_test_h, features_h = prepare_data_for_training(\n",
    "            datasets['housing'], 'MEDV'\n",
    "        )\n",
    "        prepared_data['housing'] = {\n",
    "            'X_train': X_train_h, 'X_test': X_test_h,\n",
    "            'y_train': y_train_h, 'y_test': y_test_h,\n",
    "            'features': features_h, 'target': 'MEDV',\n",
    "            'task_type': 'regression'\n",
    "        }\n",
    "        print(\"‚úÖ Housing data prepared successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error preparing Housing data: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Data preparation completed for {len(prepared_data)} datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Model Training Framework\n",
    "\n",
    "Let's create a comprehensive framework for training and evaluating multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModelTrainer:\n",
    "    \"\"\"Comprehensive ML Model Training and Evaluation Framework\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.best_models = {}\n",
    "        \n",
    "        # Define model configurations\n",
    "        self.classification_models = {\n",
    "            'Random Forest': {\n",
    "                'model': RandomForestClassifier(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100, 200],\n",
    "                    'max_depth': [5, 10, None],\n",
    "                    'min_samples_split': [2, 5, 10]\n",
    "                }\n",
    "            },\n",
    "            'Logistic Regression': {\n",
    "                'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1.0, 10.0],\n",
    "                    'solver': ['liblinear', 'lbfgs']\n",
    "                }\n",
    "            },\n",
    "            'SVM': {\n",
    "                'model': SVC(random_state=42, probability=True),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1.0, 10.0],\n",
    "                    'kernel': ['rbf', 'linear']\n",
    "                }\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'model': GradientBoostingClassifier(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100],\n",
    "                    'learning_rate': [0.1, 0.2],\n",
    "                    'max_depth': [3, 5]\n",
    "                }\n",
    "            },\n",
    "            'K-Nearest Neighbors': {\n",
    "                'model': KNeighborsClassifier(),\n",
    "                'params': {\n",
    "                    'n_neighbors': [3, 5, 7, 9],\n",
    "                    'weights': ['uniform', 'distance']\n",
    "                }\n",
    "            },\n",
    "            'Naive Bayes': {\n",
    "                'model': GaussianNB(),\n",
    "                'params': {}  # No hyperparameters to tune\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.regression_models = {\n",
    "            'Random Forest': {\n",
    "                'model': RandomForestRegressor(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100, 200],\n",
    "                    'max_depth': [5, 10, None],\n",
    "                    'min_samples_split': [2, 5, 10]\n",
    "                }\n",
    "            },\n",
    "            'Linear Regression': {\n",
    "                'model': LinearRegression(),\n",
    "                'params': {}  # No hyperparameters to tune\n",
    "            },\n",
    "            'Ridge Regression': {\n",
    "                'model': Ridge(random_state=42),\n",
    "                'params': {\n",
    "                    'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "                }\n",
    "            },\n",
    "            'Lasso Regression': {\n",
    "                'model': Lasso(random_state=42),\n",
    "                'params': {\n",
    "                    'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "                }\n",
    "            },\n",
    "            'SVR': {\n",
    "                'model': SVR(),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1.0, 10.0],\n",
    "                    'kernel': ['rbf', 'linear']\n",
    "                }\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'model': GradientBoostingRegressor(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100],\n",
    "                    'learning_rate': [0.1, 0.2],\n",
    "                    'max_depth': [3, 5]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_models(self, dataset_name, X_train, X_test, y_train, y_test, task_type='classification'):\n",
    "        \"\"\"Train multiple models with hyperparameter tuning\"\"\"\n",
    "        print(f\"ü§ñ Training models for {dataset_name} ({task_type})...\")\n",
    "        \n",
    "        # Select appropriate models\n",
    "        if task_type == 'classification':\n",
    "            models_config = self.classification_models\n",
    "        else:\n",
    "            models_config = self.regression_models\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for model_name, config in models_config.items():\n",
    "            print(f\"\\nüîÑ Training {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Get model and parameters\n",
    "                base_model = config['model']\n",
    "                param_grid = config['params']\n",
    "                \n",
    "                # Perform hyperparameter tuning if parameters exist\n",
    "                if param_grid:\n",
    "                    # Use GridSearchCV for hyperparameter tuning\n",
    "                    grid_search = GridSearchCV(\n",
    "                        base_model, param_grid, cv=5, \n",
    "                        scoring='accuracy' if task_type == 'classification' else 'r2',\n",
    "                        n_jobs=-1, verbose=0\n",
    "                    )\n",
    "                    grid_search.fit(X_train, y_train)\n",
    "                    best_model = grid_search.best_estimator_\n",
    "                    best_params = grid_search.best_params_\n",
    "                    cv_score = grid_search.best_score_\n",
    "                else:\n",
    "                    # No hyperparameters to tune\n",
    "                    best_model = base_model\n",
    "                    best_model.fit(X_train, y_train)\n",
    "                    best_params = {}\n",
    "                    # Calculate CV score manually\n",
    "                    cv_scores = cross_val_score(\n",
    "                        best_model, X_train, y_train, cv=5,\n",
    "                        scoring='accuracy' if task_type == 'classification' else 'r2'\n",
    "                    )\n",
    "                    cv_score = cv_scores.mean()\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = best_model.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                if task_type == 'classification':\n",
    "                    metrics = self._calculate_classification_metrics(y_test, y_pred, best_model, X_test)\n",
    "                else:\n",
    "                    metrics = self._calculate_regression_metrics(y_test, y_pred)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'model_name': model_name,\n",
    "                    'model': best_model,\n",
    "                    'best_params': best_params,\n",
    "                    'cv_score': cv_score,\n",
    "                    'predictions': y_pred,\n",
    "                    **metrics\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                # Print results\n",
    "                if task_type == 'classification':\n",
    "                    print(f\"   ‚úÖ {model_name}: Accuracy = {metrics['accuracy']:.4f}, CV = {cv_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   ‚úÖ {model_name}: R¬≤ = {metrics['r2_score']:.4f}, CV = {cv_score:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {model_name} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Store results\n",
    "        self.results[dataset_name] = results\n",
    "        \n",
    "        # Find best model\n",
    "        if results:\n",
    "            if task_type == 'classification':\n",
    "                best_result = max(results, key=lambda x: x['accuracy'])\n",
    "                print(f\"\\nüèÜ Best model: {best_result['model_name']} (Accuracy: {best_result['accuracy']:.4f})\")\n",
    "            else:\n",
    "                best_result = max(results, key=lambda x: x['r2_score'])\n",
    "                print(f\"\\nüèÜ Best model: {best_result['model_name']} (R¬≤: {best_result['r2_score']:.4f})\")\n",
    "            \n",
    "            self.best_models[dataset_name] = best_result\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_classification_metrics(self, y_true, y_pred, model, X_test):\n",
    "        \"\"\"Calculate classification metrics\"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'f1_score': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        }\n",
    "        \n",
    "        # Add ROC AUC if model supports probability prediction\n",
    "        try:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_proba = model.predict_proba(X_test)\n",
    "                if y_proba.shape[1] == 2:  # Binary classification\n",
    "                    metrics['roc_auc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
    "                else:  # Multi-class\n",
    "                    metrics['roc_auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr')\n",
    "        except:\n",
    "            metrics['roc_auc'] = None\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_regression_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate regression metrics\"\"\"\n",
    "        return {\n",
    "            'r2_score': r2_score(y_true, y_pred),\n",
    "            'mse': mean_squared_error(y_true, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mae': mean_absolute_error(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    def create_results_summary(self):\n",
    "        \"\"\"Create a comprehensive results summary\"\"\"\n",
    "        print(\"üìä MODEL TRAINING RESULTS SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        for dataset_name, results in self.results.items():\n",
    "            print(f\"\\nüéØ {dataset_name.title()} Results:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Determine task type\n",
    "            task_type = 'classification' if 'accuracy' in results[0] else 'regression'\n",
    "            \n",
    "            # Sort results by primary metric\n",
    "            if task_type == 'classification':\n",
    "                sorted_results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "                print(f\"{'Rank':<4} {'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "                print(\"-\" * 70)\n",
    "                \n",
    "                for i, result in enumerate(sorted_results, 1):\n",
    "                    print(f\"{i:<4} {result['model_name']:<20} {result['accuracy']:<10.4f} \"\n",
    "                          f\"{result['precision']:<10.4f} {result['recall']:<10.4f} {result['f1_score']:<10.4f}\")\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        'Dataset': dataset_name.title(),\n",
    "                        'Model': result['model_name'],\n",
    "                        'Rank': i,\n",
    "                        'Primary_Metric': result['accuracy'],\n",
    "                        'Task_Type': 'Classification'\n",
    "                    })\n",
    "            \n",
    "            else:  # regression\n",
    "                sorted_results = sorted(results, key=lambda x: x['r2_score'], reverse=True)\n",
    "                print(f\"{'Rank':<4} {'Model':<20} {'R¬≤':<10} {'RMSE':<10} {'MAE':<10}\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                for i, result in enumerate(sorted_results, 1):\n",
    "                    print(f\"{i:<4} {result['model_name']:<20} {result['r2_score']:<10.4f} \"\n",
    "                          f\"{result['rmse']:<10.4f} {result['mae']:<10.4f}\")\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        'Dataset': dataset_name.title(),\n",
    "                        'Model': result['model_name'],\n",
    "                        'Rank': i,\n",
    "                        'Primary_Metric': result['r2_score'],\n",
    "                        'Task_Type': 'Regression'\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = MLModelTrainer()\n",
    "print(\"‚úÖ ML Model Trainer initialized!\")\n",
    "print(f\"üìä Classification models: {len(trainer.classification_models)}\")\n",
    "print(f\"üìä Regression models: {len(trainer.regression_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö¢ Titanic Classification Training\n",
    "\n",
    "Let's train multiple classification models on the Titanic dataset and achieve our target accuracy of 89.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Titanic classification models\n",
    "if 'titanic' in prepared_data:\n",
    "    print(\"üö¢ TITANIC CLASSIFICATION TRAINING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    titanic_data = prepared_data['titanic']\n",
    "    \n",
    "    # Train all models\n",
    "    titanic_results = trainer.train_models(\n",
    "        'titanic',\n",
    "        titanic_data['X_train'],\n",
    "        titanic_data['X_test'],\n",
    "        titanic_data['y_train'],\n",
    "        titanic_data['y_test'],\n",
    "        task_type='classification'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Titanic training completed! Trained {len(titanic_results)} models.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Titanic data not available for training\")\n",
    "    titanic_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè† Housing Regression Training\n",
    "\n",
    "Now let's train regression models on the Housing dataset and achieve our target R¬≤ of 0.681."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Housing regression models\n",
    "if 'housing' in prepared_data:\n",
    "    print(\"üè† HOUSING REGRESSION TRAINING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    housing_data = prepared_data['housing']\n",
    "    \n",
    "    # Train all models\n",
    "    housing_results = trainer.train_models(\n",
    "        'housing',\n",
    "        housing_data['X_train'],\n",
    "        housing_data['X_test'],\n",
    "        housing_data['y_train'],\n",
    "        housing_data['y_test'],\n",
    "        task_type='regression'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Housing training completed! Trained {len(housing_results)} models.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Housing data not available for training\")\n",
    "    housing_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Model Performance Visualization\n",
    "\n",
    "Let's create comprehensive visualizations of our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_visualizations(trainer):\n",
    "    \"\"\"Create comprehensive performance visualizations\"\"\"\n",
    "    print(\"üìä Creating performance visualizations...\")\n",
    "    \n",
    "    # Create subplots based on number of datasets\n",
    "    n_datasets = len(trainer.results)\n",
    "    if n_datasets == 0:\n",
    "        print(\"‚ö†Ô∏è No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_datasets, figsize=(6*n_datasets, 12))\n",
    "    if n_datasets == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    col_idx = 0\n",
    "    \n",
    "    for dataset_name, results in trainer.results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        # Determine task type\n",
    "        task_type = 'classification' if 'accuracy' in results[0] else 'regression'\n",
    "        \n",
    "        # Extract model names and scores\n",
    "        model_names = [r['model_name'] for r in results]\n",
    "        \n",
    "        if task_type == 'classification':\n",
    "            primary_scores = [r['accuracy'] for r in results]\n",
    "            secondary_scores = [r['f1_score'] for r in results]\n",
    "            primary_label = 'Accuracy'\n",
    "            secondary_label = 'F1-Score'\n",
    "        else:\n",
    "            primary_scores = [r['r2_score'] for r in results]\n",
    "            secondary_scores = [r['rmse'] for r in results]\n",
    "            primary_label = 'R¬≤ Score'\n",
    "            secondary_label = 'RMSE'\n",
    "        \n",
    "        # Primary metric bar plot\n",
    "        ax1 = axes[0, col_idx]\n",
    "        bars1 = ax1.bar(range(len(model_names)), primary_scores, \n",
    "                       color=plt.cm.Set3(np.linspace(0, 1, len(model_names))))\n",
    "        ax1.set_title(f'{dataset_name.title()} - {primary_label}')\n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel(primary_label)\n",
    "        ax1.set_xticks(range(len(model_names)))\n",
    "        ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars1, primary_scores):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Secondary metric bar plot\n",
    "        ax2 = axes[1, col_idx]\n",
    "        bars2 = ax2.bar(range(len(model_names)), secondary_scores,\n",
    "                       color=plt.cm.Set2(np.linspace(0, 1, len(model_names))))\n",
    "        ax2.set_title(f'{dataset_name.title()} - {secondary_label}')\n",
    "        ax2.set_xlabel('Models')\n",
    "        ax2.set_ylabel(secondary_label)\n",
    "        ax2.set_xticks(range(len(model_names)))\n",
    "        ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars2, secondary_scores):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(secondary_scores)*0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        col_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create confusion matrices for classification tasks\n",
    "    classification_datasets = [name for name, results in trainer.results.items() \n",
    "                             if results and 'accuracy' in results[0]]\n",
    "    \n",
    "    if classification_datasets:\n",
    "        print(\"\\nüìä Creating confusion matrices...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(classification_datasets), \n",
    "                               figsize=(6*len(classification_datasets), 5))\n",
    "        if len(classification_datasets) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, dataset_name in enumerate(classification_datasets):\n",
    "            # Get best model results\n",
    "            best_result = trainer.best_models[dataset_name]\n",
    "            \n",
    "            # Get actual vs predicted\n",
    "            dataset_info = prepared_data[dataset_name]\n",
    "            y_true = dataset_info['y_test']\n",
    "            y_pred = best_result['predictions']\n",
    "            \n",
    "            # Create confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            \n",
    "            # Plot\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "            axes[idx].set_title(f'{dataset_name.title()} - {best_result[\"model_name\"]}\\nAccuracy: {best_result[\"accuracy\"]:.4f}')\n",
    "            axes[idx].set_xlabel('Predicted')\n",
    "            axes[idx].set_ylabel('Actual')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "if trainer.results:\n",
    "    create_performance_visualizations(trainer)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Detailed Model Analysis\n",
    "\n",
    "Let's analyze our best models in detail and create comprehensive reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_best_models(trainer, prepared_data):\n",
    "    \"\"\"Detailed analysis of best performing models\"\"\"\n",
    "    print(\"üîç DETAILED MODEL ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for dataset_name, best_result in trainer.best_models.items():\n",
    "        print(f\"\\nüèÜ Best Model for {dataset_name.title()}: {best_result['model_name']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get dataset info\n",
    "        dataset_info = prepared_data[dataset_name]\n",
    "        task_type = dataset_info['task_type']\n",
    "        \n",
    "        # Model details\n",
    "        print(f\"üìä Model Details:\")\n",
    "        print(f\"   Algorithm: {best_result['model_name']}\")\n",
    "        print(f\"   Task Type: {task_type.title()}\")\n",
    "        print(f\"   Best Parameters: {best_result['best_params']}\")\n",
    "        print(f\"   Cross-Validation Score: {best_result['cv_score']:.4f}\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        print(f\"\\nüìà Performance Metrics:\")\n",
    "        if task_type == 'classification':\n",
    "            print(f\"   Accuracy: {best_result['accuracy']:.4f}\")\n",
    "            print(f\"   Precision: {best_result['precision']:.4f}\")\n",
    "            print(f\"   Recall: {best_result['recall']:.4f}\")\n",
    "            print(f\"   F1-Score: {best_result['f1_score']:.4f}\")\n",
    "            if best_result.get('roc_auc'):\n",
    "                print(f\"   ROC AUC: {best_result['roc_auc']:.4f}\")\n",
    "        else:\n",
    "            print(f\"   R¬≤ Score: {best_result['r2_score']:.4f}\")\n",
    "            print(f\"   RMSE: {best_result['rmse']:.4f}\")\n",
    "            print(f\"   MAE: {best_result['mae']:.4f}\")\n",
    "            print(f\"   MSE: {best_result['mse']:.4f}\")\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        model = best_result['model']\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            print(f\"\\nüéØ Top 10 Most Important Features:\")\n",
    "            feature_names = dataset_info['features']\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            feature_importance = list(zip(feature_names, importances))\n",
    "            feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for i, (feature, importance) in enumerate(feature_importance[:10], 1):\n",
    "                print(f\"   {i:2d}. {feature:<25} {importance:.4f}\")\n",
    "        \n",
    "        # Model complexity\n",
    "        print(f\"\\nüîß Model Complexity:\")\n",
    "        print(f\"   Training samples: {len(dataset_info['X_train'])}\")\n",
    "        print(f\"   Test samples: {len(dataset_info['X_test'])}\")\n",
    "        print(f\"   Features: {len(dataset_info['features'])}\")\n",
    "        \n",
    "        # Prediction examples\n",
    "        print(f\"\\nüîÆ Sample Predictions:\")\n",
    "        y_true = dataset_info['y_test']\n",
    "        y_pred = best_result['predictions']\n",
    "        \n",
    "        # Show first 5 predictions\n",
    "        for i in range(min(5, len(y_true))):\n",
    "            if task_type == 'classification':\n",
    "                result_emoji = \"‚úÖ\" if y_true.iloc[i] == y_pred[i] else \"‚ùå\"\n",
    "                print(f\"   {result_emoji} Actual: {y_true.iloc[i]}, Predicted: {y_pred[i]}\")\n",
    "            else:\n",
    "                error = abs(y_true.iloc[i] - y_pred[i])\n",
    "                print(f\"   üìä Actual: {y_true.iloc[i]:.2f}, Predicted: {y_pred[i]:.2f}, Error: {error:.2f}\")\n",
    "\n",
    "# Analyze best models\n",
    "if trainer.best_models and prepared_data:\n",
    "    analyze_best_models(trainer, prepared_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No best models available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Trained Models\n",
    "\n",
    "Let's save our best models for future use and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_models(trainer, output_dir='trained_models'):\n",
    "    \"\"\"Save trained models and create model registry\"\"\"\n",
    "    print(f\"üíæ Saving trained models to {output_dir}/...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    saved_models = []\n",
    "    model_registry = []\n",
    "    \n",
    "    # Save best models\n",
    "    for dataset_name, best_result in trainer.best_models.items():\n",
    "        model_name = best_result['model_name'].lower().replace(' ', '_')\n",
    "        filename = f\"{dataset_name}_{model_name}.joblib\"\n",
    "        filepath = output_path / filename\n",
    "        \n",
    "        # Save model\n",
    "        joblib.dump(best_result['model'], filepath)\n",
    "        saved_models.append(filepath)\n",
    "        \n",
    "        # Create registry entry\n",
    "        registry_entry = {\n",
    "            'dataset': dataset_name,\n",
    "            'model_name': best_result['model_name'],\n",
    "            'algorithm': best_result['model_name'],\n",
    "            'filename': filename,\n",
    "            'filepath': str(filepath),\n",
    "            'best_params': best_result['best_params'],\n",
    "            'cv_score': best_result['cv_score'],\n",
    "            'created_date': datetime.now().isoformat(),\n",
    "            'model_size_mb': filepath.stat().st_size / (1024 * 1024) if filepath.exists() else 0\n",
    "        }\n",
    "        \n",
    "        # Add performance metrics\n",
    "        if 'accuracy' in best_result:\n",
    "            registry_entry.update({\n",
    "                'task_type': 'classification',\n",
    "                'accuracy': best_result['accuracy'],\n",
    "                'precision': best_result['precision'],\n",
    "                'recall': best_result['recall'],\n",
    "                'f1_score': best_result['f1_score']\n",
    "            })\n",
    "        else:\n",
    "            registry_entry.update({\n",
    "                'task_type': 'regression',\n",
    "                'r2_score': best_result['r2_score'],\n",
    "                'rmse': best_result['rmse'],\n",
    "                'mae': best_result['mae']\n",
    "            })\n",
    "        \n",
    "        model_registry.append(registry_entry)\n",
    "        \n",
    "        print(f\"   ‚úÖ Saved {best_result['model_name']} for {dataset_name}: {filepath}\")\n",
    "    \n",
    "    # Save all models (not just best ones)\n",
    "    print(f\"\\nüíæ Saving all trained models...\")\n",
    "    all_models_saved = 0\n",
    "    \n",
    "    for dataset_name, results in trainer.results.items():\n",
    "        for result in results:\n",
    "            model_name = result['model_name'].lower().replace(' ', '_')\n",
    "            filename = f\"{dataset_name}_{model_name}_all.joblib\"\n",
    "            filepath = output_path / filename\n",
    "            \n",
    "            # Save model\n",
    "            joblib.dump(result['model'], filepath)\n",
    "            all_models_saved += 1\n",
    "    \n",
    "    print(f\"   ‚úÖ Saved {all_models_saved} additional models\")\n",
    "    \n",
    "    # Save model registry\n",
    "    registry_file = output_path / 'model_registry.json'\n",
    "    with open(registry_file, 'w') as f:\n",
    "        json.dump(model_registry, f, indent=2)\n",
    "    \n",
    "    print(f\"   ‚úÖ Model registry saved: {registry_file}\")\n",
    "    \n",
    "    # Create training summary\n",
    "    summary_file = output_path / 'training_summary.md'\n",
    "    create_training_summary(trainer, summary_file)\n",
    "    \n",
    "    print(f\"\\nüéâ Model saving completed!\")\n",
    "    print(f\"üìÅ Saved {len(saved_models)} best models\")\n",
    "    print(f\"üìÅ Saved {all_models_saved} total models\")\n",
    "    print(f\"üìÑ Created model registry and summary\")\n",
    "    \n",
    "    return saved_models, model_registry\n",
    "\n",
    "def create_training_summary(trainer, output_file):\n",
    "    \"\"\"Create a comprehensive training summary report\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"# Model Training Summary Report\\n\\n\")\n",
    "        f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        # Overall summary\n",
    "        f.write(\"## üéØ Training Overview\\n\\n\")\n",
    "        f.write(f\"- **Datasets trained**: {len(trainer.results)}\\n\")\n",
    "        total_models = sum(len(results) for results in trainer.results.values())\n",
    "        f.write(f\"- **Total models trained**: {total_models}\\n\")\n",
    "        f.write(f\"- **Best models selected**: {len(trainer.best_models)}\\n\\n\")\n",
    "        \n",
    "        # Dataset-specific results\n",
    "        for dataset_name, results in trainer.results.items():\n",
    "            f.write(f\"## üìä {dataset_name.title()} Results\\n\\n\")\n",
    "            \n",
    "            # Task type\n",
    "            task_type = 'Classification' if 'accuracy' in results[0] else 'Regression'\n",
    "            f.write(f\"**Task Type**: {task_type}\\n\\n\")\n",
    "            \n",
    "            # Best model\n",
    "            if dataset_name in trainer.best_models:\n",
    "                best = trainer.best_models[dataset_name]\n",
    "                f.write(f\"**üèÜ Best Model**: {best['model_name']}\\n\")\n",
    "                \n",
    "                if task_type == 'Classification':\n",
    "                    f.write(f\"- **Accuracy**: {best['accuracy']:.4f}\\n\")\n",
    "                    f.write(f\"- **Precision**: {best['precision']:.4f}\\n\")\n",
    "                    f.write(f\"- **Recall**: {best['recall']:.4f}\\n\")\n",
    "                    f.write(f\"- **F1-Score**: {best['f1_score']:.4f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"- **R¬≤ Score**: {best['r2_score']:.4f}\\n\")\n",
    "                    f.write(f\"- **RMSE**: {best['rmse']:.4f}\\n\")\n",
    "                    f.write(f\"- **MAE**: {best['mae']:.4f}\\n\")\n",
    "                \n",
    "                f.write(f\"- **Cross-Validation Score**: {best['cv_score']:.4f}\\n\")\n",
    "                f.write(f\"- **Best Parameters**: {best['best_params']}\\n\\n\")\n",
    "            \n",
    "            # All models performance\n",
    "            f.write(\"### All Models Performance\\n\\n\")\n",
    "            f.write(\"| Rank | Model | \")\n",
    "            \n",
    "            if task_type == 'Classification':\n",
    "                f.write(\"Accuracy | Precision | Recall | F1-Score |\\n\")\n",
    "                f.write(\"|------|-------|----------|-----------|--------|----------|\\n\")\n",
    "                \n",
    "                sorted_results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "                for i, result in enumerate(sorted_results, 1):\n",
    "                    f.write(f\"| {i} | {result['model_name']} | {result['accuracy']:.4f} | \"\n",
    "                           f\"{result['precision']:.4f} | {result['recall']:.4f} | {result['f1_score']:.4f} |\\n\")\n",
    "            else:\n",
    "                f.write(\"R¬≤ Score | RMSE | MAE |\\n\")\n",
    "                f.write(\"|------|-------|------|-----|\\n\")\n",
    "                \n",
    "                sorted_results = sorted(results, key=lambda x: x['r2_score'], reverse=True)\n",
    "                for i, result in enumerate(sorted_results, 1):\n",
    "                    f.write(f\"| {i} | {result['model_name']} | {result['r2_score']:.4f} | \"\n",
    "                           f\"{result['rmse']:.4f} | {result['mae']:.4f} |\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Target achievement\n",
    "        f.write(\"## üéØ Target Achievement\\n\\n\")\n",
    "        \n",
    "        if 'titanic' in trainer.best_models:\n",
    "            titanic_acc = trainer.best_models['titanic']['accuracy']\n",
    "            target_acc = 0.894\n",
    "            achievement = \"‚úÖ ACHIEVED\" if titanic_acc >= target_acc else \"‚ö†Ô∏è CLOSE\"\n",
    "            f.write(f\"**Titanic Classification Target**: 89.4% accuracy\\n\")\n",
    "            f.write(f\"**Achieved**: {titanic_acc:.1%} - {achievement}\\n\\n\")\n",
    "        \n",
    "        if 'housing' in trainer.best_models:\n",
    "            housing_r2 = trainer.best_models['housing']['r2_score']\n",
    "            target_r2 = 0.681\n",
    "            achievement = \"‚úÖ ACHIEVED\" if housing_r2 >= target_r2 else \"‚ö†Ô∏è CLOSE\"\n",
    "            f.write(f\"**Housing Regression Target**: R¬≤ = 0.681\\n\")\n",
    "            f.write(f\"**Achieved**: {housing_r2:.3f} - {achievement}\\n\\n\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Training summary saved: {output_file}\")\n",
    "\n",
    "# Save models\n",
    "if trainer.best_models:\n",
    "    saved_models, model_registry = save_trained_models(trainer)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No trained models to save\")\n",
    "    saved_models, model_registry = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Results Summary\n",
    "\n",
    "Let's create a comprehensive summary of our training results and achievements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final results summary\n",
    "if trainer.results:\n",
    "    print(\"üìä FINAL TRAINING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create and display summary DataFrame\n",
    "    summary_df = trainer.create_results_summary()\n",
    "    \n",
    "    # Achievement analysis\n",
    "    print(\"\\nüéØ TARGET ACHIEVEMENT ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    targets = {\n",
    "        'titanic': {'metric': 'accuracy', 'target': 0.894, 'name': 'Titanic Classification'},\n",
    "        'housing': {'metric': 'r2_score', 'target': 0.681, 'name': 'Housing Regression'}\n",
    "    }\n",
    "    \n",
    "    achievements = []\n",
    "    \n",
    "    for dataset_name, target_info in targets.items():\n",
    "        if dataset_name in trainer.best_models:\n",
    "            best_model = trainer.best_models[dataset_name]\n",
    "            achieved_score = best_model.get(target_info['metric'], 0)\n",
    "            target_score = target_info['target']\n",
    "            \n",
    "            achievement_pct = (achieved_score / target_score) * 100\n",
    "            status = \"‚úÖ ACHIEVED\" if achieved_score >= target_score else \"‚ö†Ô∏è CLOSE\" if achievement_pct >= 95 else \"‚ùå NEEDS WORK\"\n",
    "            \n",
    "            print(f\"\\nüìä {target_info['name']}:\")\n",
    "            print(f\"   üéØ Target: {target_score:.3f}\")\n",
    "            print(f\"   üèÜ Achieved: {achieved_score:.3f}\")\n",
    "            print(f\"   üìà Achievement: {achievement_pct:.1f}% - {status}\")\n",
    "            print(f\"   ü§ñ Best Model: {best_model['model_name']}\")\n",
    "            \n",
    "            achievements.append({\n",
    "                'Dataset': target_info['name'],\n",
    "                'Target': target_score,\n",
    "                'Achieved': achieved_score,\n",
    "                'Achievement_%': achievement_pct,\n",
    "                'Status': status,\n",
    "                'Best_Model': best_model['model_name']\n",
    "            })\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"\\nüìà OVERALL TRAINING STATISTICS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    total_models = sum(len(results) for results in trainer.results.values())\n",
    "    successful_datasets = len([d for d in achievements if 'ACHIEVED' in d['Status']])\n",
    "    \n",
    "    print(f\"üìä Datasets processed: {len(trainer.results)}\")\n",
    "    print(f\"ü§ñ Total models trained: {total_models}\")\n",
    "    print(f\"üèÜ Best models selected: {len(trainer.best_models)}\")\n",
    "    print(f\"‚úÖ Targets achieved: {successful_datasets}/{len(achievements)}\")\n",
    "    print(f\"üíæ Models saved: {len(saved_models)}\")\n",
    "    \n",
    "    # Model performance ranges\n",
    "    if summary_df is not None and not summary_df.empty:\n",
    "        print(f\"\\nüìä Performance Ranges:\")\n",
    "        for dataset in summary_df['Dataset'].unique():\n",
    "            dataset_results = summary_df[summary_df['Dataset'] == dataset]\n",
    "            min_score = dataset_results['Primary_Metric'].min()\n",
    "            max_score = dataset_results['Primary_Metric'].max()\n",
    "            avg_score = dataset_results['Primary_Metric'].mean()\n",
    "            \n",
    "            metric_name = 'Accuracy' if dataset_results['Task_Type'].iloc[0] == 'Classification' else 'R¬≤ Score'\n",
    "            print(f\"   {dataset}: {metric_name} range [{min_score:.3f} - {max_score:.3f}], avg: {avg_score:.3f}\")\n",
    "    \n",
    "    print(\"\\nüéâ MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"üìÅ Check the 'trained_models/' directory for saved models\")\n",
    "    print(\"üìÑ Check 'training_summary.md' for detailed report\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training results available for final summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed the model training tutorial! You now understand:\n",
    "\n",
    "‚úÖ **Multi-Algorithm Training**: Trained 6+ different algorithms  \n",
    "‚úÖ **Hyperparameter Tuning**: Optimized model parameters with GridSearchCV  \n",
    "‚úÖ **Cross-Validation**: Robust model evaluation with 5-fold CV  \n",
    "‚úÖ **Performance Metrics**: Comprehensive evaluation (accuracy, precision, recall, F1, R¬≤, RMSE)  \n",
    "‚úÖ **Model Comparison**: Systematic comparison and selection  \n",
    "‚úÖ **Model Persistence**: Saved models for production use  \n",
    "\n",
    "### üèÜ What We Achieved\n",
    "\n",
    "**üö¢ Titanic Classification:**\n",
    "- **Target**: 89.4% accuracy\n",
    "- **Best Model**: Logistic Regression (or your best performing model)\n",
    "- **Algorithms Tested**: Random Forest, Logistic Regression, SVM, Gradient Boosting, KNN, Naive Bayes\n",
    "- **Features Used**: Engineered features from previous tutorial\n",
    "\n",
    "**üè† Housing Regression:**\n",
    "- **Target**: R¬≤ = 0.681\n",
    "- **Best Model**: Linear Regression (or your best performing model)\n",
    "- **Algorithms Tested**: Random Forest, Linear Regression, Ridge, Lasso, SVR, Gradient Boosting\n",
    "- **Features Used**: Engineered features from previous tutorial\n",
    "\n",
    "### üîß Key Techniques Mastered\n",
    "\n",
    "1. **Comprehensive Model Training**: Multiple algorithms with proper evaluation\n",
    "2. **Hyperparameter Optimization**: GridSearchCV for optimal parameters\n",
    "3. **Cross-Validation**: Robust performance estimation\n",
    "4. **Model Selection**: Data-driven best model identification\n",
    "5. **Performance Visualization**: Clear comparison charts and confusion matrices\n",
    "6. **Model Persistence**: Professional model saving and registry\n",
    "\n",
    "### üöÄ Next Tutorial\n",
    "In the next notebook (`04_experiment_tracking.ipynb`), we'll learn to:\n",
    "- Set up MLflow experiment tracking\n",
    "- Log parameters, metrics, and artifacts\n",
    "- Compare experiments across runs\n",
    "- Create experiment dashboards\n",
    "- Manage model versions\n",
    "\n",
    "### üí° Practice Exercises\n",
    "Try these exercises to reinforce your learning:\n",
    "1. Add more algorithms (XGBoost, LightGBM)\n",
    "2. Implement RandomizedSearchCV for faster tuning\n",
    "3. Create ensemble models combining best performers\n",
    "4. Experiment with different cross-validation strategies\n",
    "\n",
    "### üìÅ Files Created\n",
    "Your trained models and reports are saved in:\n",
    "- `trained_models/` - All trained models\n",
    "- `trained_models/model_registry.json` - Model metadata\n",
    "- `trained_models/training_summary.md` - Detailed report\n",
    "\n",
    "These models are ready for deployment and production use! üéä\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Ready for Experiment Tracking?**  \n",
    "Run: `jupyter notebook notebooks/04_experiment_tracking.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}